{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-17T21:40:37.094122Z",
     "start_time": "2025-07-17T21:40:37.053516Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_feature_file(features_filepath, invalid_list_filepath, output_filepath, id_column_name):\n",
    "    \"\"\"\n",
    "    Removes rows from a feature file by matching the last number of an identifier\n",
    "    against the values in the specified ID column.\n",
    "\n",
    "    Args:\n",
    "        features_filepath (str): Path to the main CSV feature file.\n",
    "        invalid_list_filepath (str): Path to the .txt file with invalid IDs.\n",
    "        output_filepath (str): Path to save the new, cleaned CSV file.\n",
    "        id_column_name (str): The name of the column in the CSV that contains the turn ID numbers.\n",
    "    \"\"\"\n",
    "    print(\"Starting the data cleaning process with the new matching logic...\")\n",
    "\n",
    "    # 1. Load the features CSV file\n",
    "    try:\n",
    "        features_df = pd.read_csv(features_filepath)\n",
    "        print(f\"✅ Successfully loaded feature file: {os.path.basename(features_filepath)}\")\n",
    "        print(f\"   Original number of rows: {len(features_df)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: The feature file was not found at '{features_filepath}'\")\n",
    "        return\n",
    "\n",
    "    # 2. Load the invalid identifiers and extract ONLY THE LAST NUMBER\n",
    "    invalid_ids = set()\n",
    "    try:\n",
    "        with open(invalid_list_filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    # Split the line by '_' and take the last part, then convert to an integer\n",
    "                    try:\n",
    "                        turn_id_to_remove = int(line.strip().split('_')[-1])\n",
    "                        invalid_ids.add(turn_id_to_remove)\n",
    "                    except (ValueError, IndexError):\n",
    "                        print(f\"  - Warning: Could not parse ID from line: '{line.strip()}'\")\n",
    "\n",
    "        print(f\"✅ Successfully extracted {len(invalid_ids)} invalid Turn IDs to remove.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: The invalid list was not found at '{invalid_list_filepath}'\")\n",
    "        return\n",
    "\n",
    "    # 3. Check if the identifier column exists\n",
    "    if id_column_name not in features_df.columns:\n",
    "        print(f\"❌ Error: Column '{id_column_name}' not found in the CSV file.\")\n",
    "        print(f\"   Available columns are: {list(features_df.columns)}\")\n",
    "        return\n",
    "\n",
    "    # 4. Filter the DataFrame using the set of invalid numbers\n",
    "    original_rows = len(features_df)\n",
    "    cleaned_df = features_df[~features_df[id_column_name].isin(invalid_ids)]\n",
    "    rows_removed = original_rows - len(cleaned_df)\n",
    "\n",
    "    print(f\"\\nFiltering complete.\")\n",
    "    print(f\"   {rows_removed} rows were removed.\")\n",
    "    print(f\"   Number of rows after cleaning: {len(cleaned_df)}\")\n",
    "\n",
    "    # 5. Save the cleaned DataFrame to a new CSV file\n",
    "    cleaned_df.to_csv(output_filepath, index=False)\n",
    "    print(f\"✅ Successfully saved cleaned data to: {output_filepath}\")\n",
    "\n",
    "\n",
    "# --- SCRIPT CONFIGURATION ---\n",
    "identifier_column = 'Turn ID'\n",
    "\n",
    "# --- FILE PATHS ---\n",
    "features_file = r'D:\\Users\\Kaiyang\\OneDrive\\The University of Colorado Denver\\OneDrive - The University of Colorado Denver\\Courses\\thesis\\data_exploration\\chat_gpt_test\\turning_project_2d_features.csv'\n",
    "invalid_ids_file = r'D:\\Users\\Kaiyang\\OneDrive\\The University of Colorado Denver\\OneDrive - The University of Colorado Denver\\Thesis\\Code\\DataExploration\\invalid_data_list.txt'\n",
    "cleaned_output_file = r'D:\\Users\\Kaiyang\\OneDrive\\The University of Colorado Denver\\OneDrive - The University of Colorado Denver\\Courses\\thesis\\data_exploration\\chat_gpt_test\\cleaned_turning_project_2d_features.csv'\n",
    "\n",
    "\n",
    "# --- RUN THE SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    clean_feature_file(\n",
    "        features_filepath=features_file,\n",
    "        invalid_list_filepath=invalid_ids_file,\n",
    "        output_filepath=cleaned_output_file,\n",
    "        id_column_name=identifier_column\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the data cleaning process with the new matching logic...\n",
      "✅ Successfully loaded feature file: turning_project_2d_features.csv\n",
      "   Original number of rows: 1749\n",
      "✅ Successfully extracted 68 invalid Turn IDs to remove.\n",
      "\n",
      "Filtering complete.\n",
      "   68 rows were removed.\n",
      "   Number of rows after cleaning: 1681\n",
      "✅ Successfully saved cleaned data to: D:\\Users\\Kaiyang\\OneDrive\\The University of Colorado Denver\\OneDrive - The University of Colorado Denver\\Courses\\thesis\\data_exploration\\chat_gpt_test\\cleaned_turning_project_2d_features.csv\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T22:26:40.622045Z",
     "start_time": "2025-07-17T22:26:40.607046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# --- Define the file names ---\n",
    "sits_labels_path = 'SitToStand_human_labels.csv'\n",
    "invalid_list_path = 'invalid_data_list.txt'\n",
    "output_csv_path = 'invalid_participants_data.csv'\n",
    "\n",
    "def filter_by_invalid_participants(sits_path, invalid_path, output_path):\n",
    "    \"\"\"\n",
    "    Filters a CSV to keep only rows corresponding to participant IDs\n",
    "    found in an invalid list.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Filtering Process ---\")\n",
    "\n",
    "    # 1. Load the source data CSV\n",
    "    try:\n",
    "        source_df = pd.read_csv(sits_path)\n",
    "        print(f\"✅ Successfully loaded source data: '{os.path.basename(sits_path)}'\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: The file was not found. Please make sure '{sits_path}' is in the same folder as the script.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing '{sits_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Load the invalid list and extract the Participant ID number\n",
    "    invalid_participant_ids = set()\n",
    "    try:\n",
    "        with open(invalid_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    # Use a regular expression to find the number immediately after \"Pt\"\n",
    "                    match = re.search(r'Pt(\\d+)', line, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        participant_id = int(match.group(1))\n",
    "                        invalid_participant_ids.add(participant_id)\n",
    "\n",
    "        print(f\"✅ Successfully extracted {len(invalid_participant_ids)} unique Participant IDs from '{os.path.basename(invalid_path)}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: The file was not found. Please make sure '{invalid_path}' is in the same folder as the script.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing '{invalid_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # 3. Define the column to match on and filter the DataFrame\n",
    "    id_column_name = 'Participant ID number'\n",
    "    if id_column_name not in source_df.columns:\n",
    "        print(f\"❌ Error: Column '{id_column_name}' not found in the source CSV.\")\n",
    "        return\n",
    "\n",
    "    # Keep only the rows where the 'Participant ID number' is in our invalid set\n",
    "    filtered_df = source_df[source_df[id_column_name].isin(invalid_participant_ids)]\n",
    "\n",
    "    # 4. Save the new CSV file\n",
    "    filtered_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n--- Results ---\")\n",
    "    print(f\"✅ Successfully saved the filtered data for {len(filtered_df)} rows to '{output_path}'.\")\n",
    "\n",
    "\n",
    "# --- RUN THE SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    filter_by_invalid_participants(\n",
    "        sits_path=sits_labels_path,\n",
    "        invalid_path=invalid_list_path,\n",
    "        output_path=output_csv_path\n",
    "    )"
   ],
   "id": "2292aa1bb0f7366e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Filtering Process ---\n",
      "✅ Successfully loaded source data: 'SitToStand_human_labels.csv'\n",
      "✅ Successfully extracted 20 unique Participant IDs from 'invalid_data_list.txt'.\n",
      "\n",
      "--- Results ---\n",
      "✅ Successfully saved the filtered data for 329 rows to 'invalid_participants_data.csv'.\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
