{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-14T19:46:45.054604Z",
     "start_time": "2025-08-14T19:46:43.369045Z"
    }
   },
   "source": [
    "# Imports & Config\n",
    "import os, re, math, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths (edit as needed)\n",
    "GROUPED_DIR = Path(r\"D:\\Courses\\thesis\\data\\turning_keypoints_grouped\")   # grouped by (PD_or_C / angle / type_of_turn)\n",
    "MODEL_DIR   = Path(r\"D:\\Courses\\thesis\\data\\second\\models_turning_norm_250\")         # where to save models (normalized version)\n",
    "SYN_DIR     = Path(r\"D:\\Courses\\thesis\\data\\second\\synthetic_turning_norm_250\")      # where to save generated CSVs\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SYN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data/Model params\n",
    "NUM_KPT   = 17\n",
    "INPUT_DIM = NUM_KPT * 2\n",
    "WINDOW    = 10\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS     = 250\n",
    "LR         = 1e-3\n",
    "SEED       = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "print(\"Device:\", DEVICE)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:47:46.912518Z",
     "start_time": "2025-08-14T19:47:46.906519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Skeleton graph + root joint used for normalization\n",
    "connections = [\n",
    "    [0, 1], [1, 2], [2, 3],\n",
    "    [0, 4], [4, 5], [5, 6],\n",
    "    [0, 7], [7, 8], [8, 9], [9, 10],\n",
    "    [8, 11], [11, 12], [12, 13],\n",
    "    [8, 14], [14, 15], [15, 16],\n",
    "]\n",
    "ROOT = 8  # central joint (hub) in this graph\n"
   ],
   "id": "ebb1a8ef7927115b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:47:48.452809Z",
     "start_time": "2025-08-14T19:47:48.434302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List groups and parse subject id from filename\n",
    "def list_groups(grouped_dir: Path):\n",
    "    \"\"\"Return list of (pd_or_c, angle, turn_type, path, n_files).\"\"\"\n",
    "    triples = []\n",
    "    for pdc_dir in sorted([p for p in grouped_dir.iterdir() if p.is_dir()]):\n",
    "        for angle_dir in sorted([p for p in pdc_dir.iterdir() if p.is_dir()]):\n",
    "            for ttype_dir in sorted([p for p in angle_dir.iterdir() if p.is_dir()]):\n",
    "                n = len(list(ttype_dir.glob(\"*.csv\")))\n",
    "                if n > 0:\n",
    "                    triples.append((pdc_dir.name, angle_dir.name, ttype_dir.name, ttype_dir, n))\n",
    "    return triples\n",
    "\n",
    "def parse_subject_id_from_filename(name: str):\n",
    "    # e.g., Pt204_C_n_350.csv -> 204\n",
    "    m = re.match(r\"Pt(\\d+)_\", name)\n",
    "    if m:\n",
    "        try: return int(m.group(1))\n",
    "        except: return None\n",
    "    return None\n"
   ],
   "id": "9f3485a7f7b1fb4c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:47:49.847652Z",
     "start_time": "2025-08-14T19:47:49.837650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PATCH: headerless-safe CSV loader + group loader (overrides previous versions)\n",
    "\n",
    "def _has_numeric_header(df) -> bool:\n",
    "    \"\"\"True if most column labels look like numbers (means we accidentally used row 1 as header).\"\"\"\n",
    "    if len(df.columns) == 0:\n",
    "        return False\n",
    "    num_like = 0\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            float(str(c))\n",
    "            num_like += 1\n",
    "        except:\n",
    "            pass\n",
    "    return (num_like / len(df.columns)) >= 0.8\n",
    "\n",
    "def read_keypoints_csv(path: Path, input_dim=34):\n",
    "    \"\"\"\n",
    "    Robust loader for headerless files:\n",
    "    - If the first read looks like it has numeric headers, re-read with header=None.\n",
    "    - Always select the first `input_dim` numeric columns by position.\n",
    "    - Drop NaN rows.\n",
    "    - Require at least WINDOW+2 frames.\n",
    "    - Return synthetic column names x1,y1,...,x17,y17 for consistency.\n",
    "    \"\"\"\n",
    "    # first try (might wrongly treat first row as header)\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    if _has_numeric_header(df):\n",
    "        # re-read with no header so the first row is data\n",
    "        df = pd.read_csv(path, header=None, low_memory=False)\n",
    "\n",
    "    num_df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    if num_df.shape[1] < input_dim:\n",
    "        return None\n",
    "\n",
    "    use = num_df.iloc[:, :input_dim].dropna()\n",
    "    arr = use.to_numpy(dtype=np.float32)\n",
    "    if arr.shape[0] < WINDOW + 2:\n",
    "        return None\n",
    "\n",
    "    # synthetic XY names for saving later\n",
    "    cols = []\n",
    "    for k in range(1, NUM_KPT + 1):\n",
    "        cols += [f\"x{k}\", f\"y{k}\"]\n",
    "\n",
    "    return arr, cols\n",
    "\n",
    "def load_group_sequences(group_path: Path, input_dim=34):\n",
    "    \"\"\"\n",
    "    Load all CSVs in the group using the headerless-safe reader.\n",
    "    Do NOT enforce a shared header schema; we always output the same synthetic XY names.\n",
    "    \"\"\"\n",
    "    file_list, seqs, headers, subjs = [], [], [], []\n",
    "    for f in sorted(group_path.glob(\"*.csv\")):\n",
    "        out = read_keypoints_csv(f, input_dim=input_dim)\n",
    "        if out is None:\n",
    "            continue\n",
    "        arr, cols = out\n",
    "        file_list.append(f.name)\n",
    "        seqs.append(arr)\n",
    "        headers.append(cols)  # all identical synthetic headers\n",
    "        subjs.append(parse_subject_id_from_filename(f.name))\n",
    "    return file_list, seqs, headers, subjs\n"
   ],
   "id": "e147ff5f449dedc",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:47:53.733475Z",
     "start_time": "2025-08-14T19:47:53.724474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalization utilities (sequence-level) + bone-length helpers\n",
    "def _median_bone_scale_2d(points_17x2):\n",
    "    centered = points_17x2 - points_17x2[ROOT]\n",
    "    dists = [np.linalg.norm(centered[j] - centered[i]) for i, j in connections]\n",
    "    return np.median(dists) if len(dists) else 1.0\n",
    "\n",
    "def normalize_sequence_xy(seq_Tx34):\n",
    "    \"\"\"\n",
    "    Per-sequence normalization using frame 0:\n",
    "    - subtract root joint of frame 0 (translation)\n",
    "    - divide by median bone length of frame 0 (scale)\n",
    "    Returns: seq_norm (T,34), root0 (2,), scale0 (float)\n",
    "    \"\"\"\n",
    "    T = seq_Tx34.shape[0]\n",
    "    pts0 = seq_Tx34[0].reshape(17, 2)\n",
    "    root0 = pts0[ROOT].copy()\n",
    "    s0 = _median_bone_scale_2d(pts0)\n",
    "    if not np.isfinite(s0) or s0 <= 1e-6:\n",
    "        s0 = 1.0\n",
    "\n",
    "    seq = seq_Tx34.reshape(T, 17, 2)\n",
    "    seq_centered = seq - root0\n",
    "    seq_scaled = seq_centered / s0\n",
    "    return seq_scaled.reshape(T, 34), root0, float(s0)\n",
    "\n",
    "def denormalize_frame_34(frame_34, root0, scale0):\n",
    "    pts = frame_34.reshape(17, 2) * scale0 + root0\n",
    "    return pts.reshape(34,)\n",
    "\n",
    "def bone_lengths_batch(x_B_T_34):\n",
    "    \"\"\"x: (B,T,34) in normalized coords -> (B,T, num_bones) lengths.\"\"\"\n",
    "    B, T, F = x_B_T_34.shape\n",
    "    coords = x_B_T_34.view(B, T, 17, 2)\n",
    "    lens = []\n",
    "    for (i, j) in connections:\n",
    "        diff = coords[:, :, j, :] - coords[:, :, i, :]\n",
    "        l = torch.norm(diff, dim=-1)  # (B,T)\n",
    "        lens.append(l.unsqueeze(-1))\n",
    "    return torch.cat(lens, dim=-1)\n"
   ],
   "id": "ce4b61e62158f122",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:47:57.995856Z",
     "start_time": "2025-08-14T19:47:57.982854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split (subject-aware), window count, scaler utils\n",
    "def split_subject_aware(file_list, seqs, subjs, test_size=0.15, seed=42):\n",
    "    man = pd.DataFrame({\"idx\": range(len(file_list)), \"subj\": subjs})\n",
    "    known = man[man[\"subj\"].notna()]\n",
    "    unknown = man[man[\"subj\"].isna()]\n",
    "\n",
    "    train_idx, val_idx = [], []\n",
    "\n",
    "    if len(known[\"subj\"].unique()) >= 2:\n",
    "        tr_subj, va_subj = train_test_split(sorted(known[\"subj\"].unique()),\n",
    "                                            test_size=test_size, random_state=seed, shuffle=True)\n",
    "        train_idx += known[known[\"subj\"].isin(tr_subj)][\"idx\"].tolist()\n",
    "        val_idx   += known[known[\"subj\"].isin(va_subj)][\"idx\"].tolist()\n",
    "    else:\n",
    "        if len(known) >= 2:\n",
    "            tr, va = train_test_split(known[\"idx\"].tolist(),\n",
    "                                      test_size=max(1/len(known), test_size),\n",
    "                                      random_state=seed, shuffle=True)\n",
    "            train_idx += tr; val_idx += va\n",
    "\n",
    "    if len(unknown) >= 2:\n",
    "        tr, va = train_test_split(unknown[\"idx\"].tolist(),\n",
    "                                  test_size=max(1/len(unknown), test_size),\n",
    "                                  random_state=seed, shuffle=True)\n",
    "        train_idx += tr; val_idx += va\n",
    "    else:\n",
    "        train_idx += unknown[\"idx\"].tolist()\n",
    "\n",
    "    if len(val_idx) == 0:\n",
    "        train_idx = man[\"idx\"].tolist()\n",
    "        val_idx = []\n",
    "\n",
    "    train_seqs = [seqs[i] for i in train_idx]\n",
    "    val_seqs   = [seqs[i] for i in val_idx]\n",
    "    return train_seqs, val_seqs, train_idx, val_idx\n",
    "\n",
    "def make_windows_count(seqs, win):\n",
    "    return sum(max(0, len(s)-win) for s in seqs)\n",
    "\n",
    "def scaler_from_params(mean, scale):\n",
    "    sc = StandardScaler()\n",
    "    sc.mean_ = np.array(mean, dtype=np.float64)\n",
    "    sc.scale_ = np.array(scale, dtype=np.float64)\n",
    "    sc.var_ = sc.scale_**2\n",
    "    sc.n_features_in_ = len(sc.mean_)\n",
    "    return sc\n"
   ],
   "id": "74b9eeb46104e076",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:48:03.978680Z",
     "start_time": "2025-08-14T19:48:03.959680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transformer-VAE (batch_first=True)\n",
    "class PositionalEncodingBF(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1,L,D)\n",
    "    def forward(self, x):  # x: (B,T,D)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerVAE_BF(nn.Module):\n",
    "    def __init__(self, input_dim, win=WINDOW, d_model=96, nhead=6, num_layers=3, latent_dim=48):\n",
    "        super().__init__()\n",
    "        self.win = win\n",
    "        self.input_linear = nn.Linear(input_dim, d_model)\n",
    "        self.pos_enc = PositionalEncodingBF(d_model, max_len=win)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.fc_mu = nn.Linear(d_model * win, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(d_model * win, latent_dim)\n",
    "        self.fc_latent = nn.Linear(latent_dim, d_model * win)\n",
    "        dec_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=num_layers)\n",
    "        self.output_linear = nn.Linear(d_model, input_dim)\n",
    "    def encode(self, x):\n",
    "        h = self.input_linear(x)\n",
    "        h = self.pos_enc(h)\n",
    "        out = self.encoder(h)                # (B,T,D)\n",
    "        flat = out.reshape(out.size(0), -1)\n",
    "        return self.fc_mu(flat), self.fc_logvar(flat)\n",
    "    def reparameterize(self, mu, logvar, temperature=1.0):\n",
    "        std = torch.exp(0.5 * logvar) * temperature\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def decode(self, z):\n",
    "        x = self.fc_latent(z).view(z.size(0), self.win, -1)\n",
    "        tgt = self.pos_enc(x)\n",
    "        memory = torch.zeros(z.size(0), self.win, tgt.size(2), device=z.device)  # (B,T,D)\n",
    "        out = self.decoder(tgt, memory)\n",
    "        return self.output_linear(out)\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n"
   ],
   "id": "acdde5f7934968bb",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:48:09.227875Z",
     "start_time": "2025-08-14T19:48:09.209872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss: reconstruction + KL + velocity + bone-length consistency\n",
    "def loss_with_bone(recon_x, x, mu, logvar, w_kld=0.1, w_vel=0.15, w_bone=0.15):\n",
    "    \"\"\"\n",
    "    x, recon_x: (B,T,34) in normalized coords\n",
    "    \"\"\"\n",
    "    mse = F.mse_loss(recon_x, x)\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    vel_orig = torch.diff(x, dim=1)\n",
    "    vel_recon = torch.diff(recon_x, dim=1)\n",
    "    vel = F.mse_loss(vel_recon, vel_orig)\n",
    "\n",
    "    bl_orig = bone_lengths_batch(x)\n",
    "    bl_recon = bone_lengths_batch(recon_x)\n",
    "    bone = F.mse_loss(bl_recon, bl_orig)\n",
    "\n",
    "    return mse + w_kld * kld + w_vel * vel + w_bone * bone\n"
   ],
   "id": "3666721ce8e0c6ca",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:48:15.761913Z",
     "start_time": "2025-08-14T19:48:15.742906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train one group (normalized pipeline) and save checkpoint\n",
    "class _WindowDataset(Dataset):\n",
    "    def __init__(self, seqs_norm, scaler, win):\n",
    "        xs = []\n",
    "        for s in seqs_norm:\n",
    "            s2 = scaler.transform(s)  # standardize after our normalization\n",
    "            for i in range(win, len(s2)):\n",
    "                xs.append(s2[i - win:i])\n",
    "        self.samples = np.asarray(xs, np.float32)\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i): return torch.tensor(self.samples[i], dtype=torch.float32)\n",
    "\n",
    "def train_one_group(triple, group_path: Path):\n",
    "    file_list, seqs_raw, headers, subjs = load_group_sequences(group_path, INPUT_DIM)\n",
    "    if len(seqs_raw) == 0:\n",
    "        print(f\"  [SKIP empty] {triple}\")\n",
    "        return None\n",
    "\n",
    "    # 1) Per-sequence normalization\n",
    "    seqs_norm = []\n",
    "    for arr in seqs_raw:\n",
    "        s_norm, _, _ = normalize_sequence_xy(arr)  # ignore root/scale during training\n",
    "        seqs_norm.append(s_norm)\n",
    "\n",
    "    # 2) Subject-aware split\n",
    "    _, _, train_idx, val_idx = split_subject_aware(file_list, seqs_norm, subjs, test_size=0.15, seed=SEED)\n",
    "    train_seqs = [seqs_norm[i] for i in train_idx] if len(train_idx) else seqs_norm\n",
    "    val_seqs   = [seqs_norm[i] for i in val_idx]   if len(val_idx)   else []\n",
    "\n",
    "    # enough windows?\n",
    "    if make_windows_count(train_seqs, WINDOW) < 64:\n",
    "        print(f\"  [SKIP small] {triple} — not enough windows\")\n",
    "        return None\n",
    "\n",
    "    # 3) Fit scaler on normalized train data\n",
    "    scaler = StandardScaler().fit(np.concatenate(train_seqs, axis=0))\n",
    "\n",
    "    # 4) Dataloaders\n",
    "    train_loader = DataLoader(_WindowDataset(train_seqs, scaler, WINDOW),\n",
    "                              batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(_WindowDataset(val_seqs, scaler, WINDOW),\n",
    "                            batch_size=BATCH_SIZE, shuffle=False, drop_last=False) if len(val_seqs)>0 else None\n",
    "\n",
    "    # 5) Model\n",
    "    model = TransformerVAE_BF(INPUT_DIM, win=WINDOW).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_val = float('inf'); best_state = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train(); total = 0.0\n",
    "        for b in train_loader:\n",
    "            b = b.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            recon, mu, logvar = model(b)\n",
    "            loss = loss_with_bone(recon, b, mu, logvar)\n",
    "            loss.backward(); opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        if (epoch % 5 == 0) or epoch == 1:\n",
    "            if val_loader and len(val_loader) > 0:\n",
    "                model.eval(); vtot = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for vb in val_loader:\n",
    "                        vb = vb.to(DEVICE)\n",
    "                        r, m, l = model(vb)\n",
    "                        vtot += loss_with_bone(r, vb, m, l).item()\n",
    "                vloss = vtot / len(val_loader)\n",
    "            else:\n",
    "                vloss = float('nan')\n",
    "\n",
    "            print(f\"  Epoch {epoch:02d}/{EPOCHS} — train {total/len(train_loader):.5f} val {vloss:.5f}\")\n",
    "            if not math.isnan(vloss) and vloss < best_val:\n",
    "                best_val = vloss; best_state = model.state_dict()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # save checkpoint\n",
    "    header_series = pd.Series([\"|\".join(h) for h in headers if h])\n",
    "    header_cols = header_series.mode().iloc[0].split(\"|\") if not header_series.empty \\\n",
    "                  else [f\"{ax}{i}\" for i in range(1, NUM_KPT+1) for ax in (\"x\",\"y\")]\n",
    "    tag = \"__\".join(triple)\n",
    "    ckpt = MODEL_DIR / f\"TVAE_{tag}.pt\"\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"scaler_mean\": scaler.mean_,\n",
    "        \"scaler_scale\": scaler.scale_,\n",
    "        \"header_cols\": header_cols,\n",
    "        \"meta\": {\n",
    "            \"group\": triple,\n",
    "            \"input_dim\": INPUT_DIM,\n",
    "            \"window\": WINDOW,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"best_val\": None if math.isinf(best_val) else float(best_val),\n",
    "            \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"normalized\": True,\n",
    "            \"root_index\": ROOT,\n",
    "            \"connections\": connections,\n",
    "        }\n",
    "    }, ckpt)\n",
    "    return ckpt\n"
   ],
   "id": "701b5a124b604db9",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T04:47:57.182824Z",
     "start_time": "2025-08-14T03:27:56.885426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train all groups once & write a manifest\n",
    "groups = list_groups(GROUPED_DIR)\n",
    "print(f\"Discovered {len(groups)} groups.\")\n",
    "\n",
    "manifest_rows = []\n",
    "for pd_or_c, angle, ttype, path, n_files in groups:\n",
    "    triple = (pd_or_c, angle, ttype)\n",
    "    tag = \"__\".join(triple)\n",
    "    ckpt_path = MODEL_DIR / f\"TVAE_{tag}.pt\"\n",
    "    if ckpt_path.exists():\n",
    "        print(f\"[SKIP existing] {triple}\")\n",
    "        saved = True\n",
    "        ckpt = ckpt_path\n",
    "    else:\n",
    "        print(f\"[TRAIN] {triple}  ({n_files} files)\")\n",
    "        try:\n",
    "            ckpt = train_one_group(triple, path)\n",
    "            saved = ckpt is not None\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR training {triple}: {e}\")\n",
    "            ckpt = None\n",
    "            saved = False\n",
    "\n",
    "    manifest_rows.append({\n",
    "        \"PD_or_C\": pd_or_c,\n",
    "        \"turning_angle\": angle,\n",
    "        \"type_of_turn\": ttype,\n",
    "        \"n_files\": n_files,\n",
    "        \"model_saved\": saved,\n",
    "        \"model_path\": str(ckpt) if ckpt else None,\n",
    "    })\n",
    "\n",
    "manifest = pd.DataFrame(manifest_rows)\n",
    "manifest_csv = MODEL_DIR / \"_models_manifest.csv\"\n",
    "manifest.to_csv(manifest_csv, index=False)\n",
    "print(\"\\nSaved model manifest:\", manifest_csv)\n",
    "display(manifest.head(20))\n"
   ],
   "id": "f19e982534d98c73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 21 groups.\n",
      "[TRAIN] ('C', '135_degrees', '-')  (1 files)\n",
      "  [SKIP small] ('C', '135_degrees', '-') — not enough windows\n",
      "[TRAIN] ('C', '135_degrees', 'pivot_turn')  (74 files)\n",
      "  Epoch 01/250 — train 0.90586 val 1.39194\n",
      "  Epoch 05/250 — train 0.40826 val 0.98006\n",
      "  Epoch 10/250 — train 0.38053 val 0.95755\n",
      "  Epoch 15/250 — train 0.37228 val 0.89516\n",
      "  Epoch 20/250 — train 0.35716 val 0.83862\n",
      "  Epoch 25/250 — train 0.33993 val 0.88387\n",
      "  Epoch 30/250 — train 0.38931 val 0.94608\n",
      "  Epoch 35/250 — train 0.32523 val 0.87369\n",
      "  Epoch 40/250 — train 0.32296 val 0.80235\n",
      "  Epoch 45/250 — train 0.19530 val 0.43096\n",
      "  Epoch 50/250 — train 0.16806 val 0.38778\n",
      "  Epoch 55/250 — train 0.15317 val 0.38185\n",
      "  Epoch 60/250 — train 0.15131 val 0.37865\n",
      "  Epoch 65/250 — train 0.14225 val 0.38823\n",
      "  Epoch 70/250 — train 0.14834 val 0.37127\n",
      "  Epoch 75/250 — train 0.13752 val 0.38633\n",
      "  Epoch 80/250 — train 0.12877 val 0.39096\n",
      "  Epoch 85/250 — train 0.12523 val 0.37405\n",
      "  Epoch 90/250 — train 0.12387 val 0.36287\n",
      "  Epoch 95/250 — train 0.13151 val 0.37576\n",
      "  Epoch 100/250 — train 0.13364 val 0.40343\n",
      "  Epoch 105/250 — train 0.12876 val 0.36224\n",
      "  Epoch 110/250 — train 0.12624 val 0.38969\n",
      "  Epoch 115/250 — train 0.11393 val 0.39658\n",
      "  Epoch 120/250 — train 0.13213 val 0.42959\n",
      "  Epoch 125/250 — train 0.11141 val 0.38820\n",
      "  Epoch 130/250 — train 0.13200 val 0.39310\n",
      "  Epoch 135/250 — train 0.11553 val 0.37499\n",
      "  Epoch 140/250 — train 0.12464 val 0.40932\n",
      "  Epoch 145/250 — train 0.11628 val 0.39102\n",
      "  Epoch 150/250 — train 0.12470 val 0.41523\n",
      "  Epoch 155/250 — train 0.11087 val 0.40152\n",
      "  Epoch 160/250 — train 0.11598 val 0.39325\n",
      "  Epoch 165/250 — train 0.11006 val 0.40815\n",
      "  Epoch 170/250 — train 0.10682 val 0.41064\n",
      "  Epoch 175/250 — train 0.11551 val 0.44826\n",
      "  Epoch 180/250 — train 0.11802 val 0.40516\n",
      "  Epoch 185/250 — train 0.12567 val 0.37726\n",
      "  Epoch 190/250 — train 0.11495 val 0.40730\n",
      "  Epoch 195/250 — train 0.11037 val 0.41325\n",
      "  Epoch 200/250 — train 0.11877 val 0.39439\n",
      "  Epoch 205/250 — train 0.10517 val 0.43347\n",
      "  Epoch 210/250 — train 0.10632 val 0.40370\n",
      "  Epoch 215/250 — train 0.10495 val 0.40045\n",
      "  Epoch 220/250 — train 0.12027 val 0.42489\n",
      "  Epoch 225/250 — train 0.13547 val 0.39851\n",
      "  Epoch 230/250 — train 0.11905 val 0.40532\n",
      "  Epoch 235/250 — train 0.10504 val 0.40547\n",
      "  Epoch 240/250 — train 0.10834 val 0.41379\n",
      "  Epoch 245/250 — train 0.10306 val 0.40445\n",
      "  Epoch 250/250 — train 0.11784 val 0.43549\n",
      "[TRAIN] ('C', '135_degrees', 'step_turn')  (3 files)\n",
      "  Epoch 01/250 — train 1.33746 val 2.64663\n",
      "  Epoch 05/250 — train 0.94273 val 2.24183\n",
      "  Epoch 10/250 — train 0.98717 val 2.25868\n",
      "  Epoch 15/250 — train 0.93095 val 2.15724\n",
      "  Epoch 20/250 — train 0.59512 val 2.22553\n",
      "  Epoch 25/250 — train 0.56718 val 3.12342\n",
      "  Epoch 30/250 — train 0.61165 val 2.80895\n",
      "  Epoch 35/250 — train 0.53498 val 2.52720\n",
      "  Epoch 40/250 — train 0.51316 val 2.37965\n",
      "  Epoch 45/250 — train 0.50672 val 2.35615\n",
      "  Epoch 50/250 — train 0.50813 val 2.44367\n",
      "  Epoch 55/250 — train 0.49327 val 2.47962\n",
      "  Epoch 60/250 — train 0.47192 val 2.42304\n",
      "  Epoch 65/250 — train 0.46889 val 2.41880\n",
      "  Epoch 70/250 — train 0.49959 val 2.47107\n",
      "  Epoch 75/250 — train 0.55571 val 2.42452\n",
      "  Epoch 80/250 — train 0.51614 val 2.40659\n",
      "  Epoch 85/250 — train 0.52252 val 2.45568\n",
      "  Epoch 90/250 — train 0.49918 val 2.48809\n",
      "  Epoch 95/250 — train 0.49994 val 2.45092\n",
      "  Epoch 100/250 — train 0.49793 val 2.42384\n",
      "  Epoch 105/250 — train 0.45373 val 2.42134\n",
      "  Epoch 110/250 — train 0.48513 val 2.42490\n",
      "  Epoch 115/250 — train 0.48321 val 2.41759\n",
      "  Epoch 120/250 — train 0.52350 val 2.40662\n",
      "  Epoch 125/250 — train 0.44228 val 2.39728\n",
      "  Epoch 130/250 — train 0.49047 val 2.43945\n",
      "  Epoch 135/250 — train 0.50820 val 2.49506\n",
      "  Epoch 140/250 — train 0.47343 val 2.37186\n",
      "  Epoch 145/250 — train 0.47802 val 2.43327\n",
      "  Epoch 150/250 — train 0.50751 val 2.41176\n",
      "  Epoch 155/250 — train 0.52128 val 2.46276\n",
      "  Epoch 160/250 — train 0.49797 val 2.48205\n",
      "  Epoch 165/250 — train 0.44185 val 2.39306\n",
      "  Epoch 170/250 — train 0.51509 val 2.37721\n",
      "  Epoch 175/250 — train 0.51128 val 2.42044\n",
      "  Epoch 180/250 — train 0.48393 val 2.39305\n",
      "  Epoch 185/250 — train 0.46503 val 2.42302\n",
      "  Epoch 190/250 — train 0.42668 val 2.37613\n",
      "  Epoch 195/250 — train 0.49982 val 2.58505\n",
      "  Epoch 200/250 — train 0.49995 val 2.37171\n",
      "  Epoch 205/250 — train 0.48156 val 2.41856\n",
      "  Epoch 210/250 — train 0.51704 val 2.42320\n",
      "  Epoch 215/250 — train 0.48247 val 2.38958\n",
      "  Epoch 220/250 — train 0.46222 val 2.40252\n",
      "  Epoch 225/250 — train 0.48036 val 2.43761\n",
      "  Epoch 230/250 — train 0.41487 val 2.43576\n",
      "  Epoch 235/250 — train 0.45663 val 2.25812\n",
      "  Epoch 240/250 — train 0.37102 val 2.12638\n",
      "  Epoch 245/250 — train 0.37340 val 2.16786\n",
      "  Epoch 250/250 — train 0.37619 val 2.08004\n",
      "[TRAIN] ('C', '180_degrees', 'pivot_turn')  (234 files)\n",
      "  Epoch 01/250 — train 0.50287 val 0.47001\n",
      "  Epoch 05/250 — train 0.22853 val 0.28690\n",
      "  Epoch 10/250 — train 0.15317 val 0.21883\n",
      "  Epoch 15/250 — train 0.11966 val 0.18605\n",
      "  Epoch 20/250 — train 0.10384 val 0.17081\n",
      "  Epoch 25/250 — train 0.09561 val 0.17873\n",
      "  Epoch 30/250 — train 0.09020 val 0.15209\n",
      "  Epoch 35/250 — train 0.08537 val 0.16519\n",
      "  Epoch 40/250 — train 0.08418 val 0.17553\n",
      "  Epoch 45/250 — train 0.08160 val 0.16504\n",
      "  Epoch 50/250 — train 0.07826 val 0.15334\n",
      "  Epoch 55/250 — train 0.07785 val 0.16554\n",
      "  Epoch 60/250 — train 0.07602 val 0.16877\n",
      "  Epoch 65/250 — train 0.07608 val 0.16928\n",
      "  Epoch 70/250 — train 0.07357 val 0.17119\n",
      "  Epoch 75/250 — train 0.07240 val 0.17126\n",
      "  Epoch 80/250 — train 0.07094 val 0.16946\n",
      "  Epoch 85/250 — train 0.07086 val 0.16999\n",
      "  Epoch 90/250 — train 0.06851 val 0.17490\n",
      "  Epoch 95/250 — train 0.06792 val 0.17224\n",
      "  Epoch 100/250 — train 0.06770 val 0.17912\n",
      "  Epoch 105/250 — train 0.06656 val 0.18009\n",
      "  Epoch 110/250 — train 0.06764 val 0.17106\n",
      "  Epoch 115/250 — train 0.06568 val 0.17626\n",
      "  Epoch 120/250 — train 0.06541 val 0.17509\n",
      "  Epoch 125/250 — train 0.06438 val 0.18920\n",
      "  Epoch 130/250 — train 0.06408 val 0.18245\n",
      "  Epoch 135/250 — train 0.06529 val 0.19640\n",
      "  Epoch 140/250 — train 0.06330 val 0.18687\n",
      "  Epoch 145/250 — train 0.06221 val 0.18144\n",
      "  Epoch 150/250 — train 0.06292 val 0.17284\n",
      "  Epoch 155/250 — train 0.06182 val 0.18577\n",
      "  Epoch 160/250 — train 0.06152 val 0.17728\n",
      "  Epoch 165/250 — train 0.06259 val 0.16892\n",
      "  Epoch 170/250 — train 0.06202 val 0.18169\n",
      "  Epoch 175/250 — train 0.06193 val 0.17976\n",
      "  Epoch 180/250 — train 0.06111 val 0.17654\n",
      "  Epoch 185/250 — train 0.06109 val 0.17847\n",
      "  Epoch 190/250 — train 0.06005 val 0.17595\n",
      "  Epoch 195/250 — train 0.06022 val 0.17922\n",
      "  Epoch 200/250 — train 0.06058 val 0.19108\n",
      "  Epoch 205/250 — train 0.06126 val 0.17735\n",
      "  Epoch 210/250 — train 0.05963 val 0.18560\n",
      "  Epoch 215/250 — train 0.06011 val 0.19002\n",
      "  Epoch 220/250 — train 0.05957 val 0.18633\n",
      "  Epoch 225/250 — train 0.05953 val 0.19464\n",
      "  Epoch 230/250 — train 0.05895 val 0.18250\n",
      "  Epoch 235/250 — train 0.05932 val 0.19785\n",
      "  Epoch 240/250 — train 0.05884 val 0.19703\n",
      "  Epoch 245/250 — train 0.05899 val 0.19681\n",
      "  Epoch 250/250 — train 0.05821 val 0.19352\n",
      "[TRAIN] ('C', '180_degrees', 'step_turn')  (12 files)\n",
      "  Epoch 01/250 — train 1.22391 val 41.70906\n",
      "  Epoch 05/250 — train 0.97322 val 40.37461\n",
      "  Epoch 10/250 — train 0.95196 val 39.41227\n",
      "  Epoch 15/250 — train 0.57256 val 37.53551\n",
      "  Epoch 20/250 — train 0.59146 val 37.07589\n",
      "  Epoch 25/250 — train 0.59001 val 36.72853\n",
      "  Epoch 30/250 — train 0.51765 val 36.42306\n",
      "  Epoch 35/250 — train 0.56865 val 36.31667\n",
      "  Epoch 40/250 — train 0.50647 val 36.31125\n",
      "  Epoch 45/250 — train 0.50443 val 36.20213\n",
      "  Epoch 50/250 — train 0.53298 val 36.10066\n",
      "  Epoch 55/250 — train 0.45823 val 35.74698\n",
      "  Epoch 60/250 — train 0.43682 val 35.90709\n",
      "  Epoch 65/250 — train 0.43915 val 36.15526\n",
      "  Epoch 70/250 — train 0.41592 val 36.09832\n",
      "  Epoch 75/250 — train 0.42713 val 35.40185\n",
      "  Epoch 80/250 — train 0.41166 val 35.39611\n",
      "  Epoch 85/250 — train 0.43734 val 35.92420\n",
      "  Epoch 90/250 — train 0.43256 val 35.23694\n",
      "  Epoch 95/250 — train 0.38770 val 36.00094\n",
      "  Epoch 100/250 — train 0.38455 val 35.80871\n",
      "  Epoch 105/250 — train 0.41892 val 34.76501\n",
      "  Epoch 110/250 — train 0.41262 val 36.09444\n",
      "  Epoch 115/250 — train 0.39153 val 35.37512\n",
      "  Epoch 120/250 — train 0.35473 val 36.00757\n",
      "  Epoch 125/250 — train 0.34431 val 35.32511\n",
      "  Epoch 130/250 — train 0.35243 val 35.24709\n",
      "  Epoch 135/250 — train 0.31923 val 35.33633\n",
      "  Epoch 140/250 — train 0.32832 val 35.78138\n",
      "  Epoch 145/250 — train 0.28292 val 35.62809\n",
      "  Epoch 150/250 — train 0.27052 val 35.08214\n",
      "  Epoch 155/250 — train 0.27154 val 35.53862\n",
      "  Epoch 160/250 — train 0.29278 val 35.29248\n",
      "  Epoch 165/250 — train 0.29976 val 36.65564\n",
      "  Epoch 170/250 — train 0.28754 val 35.18979\n",
      "  Epoch 175/250 — train 0.30021 val 36.48136\n",
      "  Epoch 180/250 — train 0.27972 val 36.10432\n",
      "  Epoch 185/250 — train 0.27737 val 36.41806\n",
      "  Epoch 190/250 — train 0.29079 val 36.14203\n",
      "  Epoch 195/250 — train 0.29139 val 37.12109\n",
      "  Epoch 200/250 — train 0.28155 val 37.74586\n",
      "  Epoch 205/250 — train 0.28162 val 39.88160\n",
      "  Epoch 210/250 — train 0.26342 val 36.99421\n",
      "  Epoch 215/250 — train 0.24019 val 34.96019\n",
      "  Epoch 220/250 — train 0.26970 val 35.01442\n",
      "  Epoch 225/250 — train 0.24737 val 34.77767\n",
      "  Epoch 230/250 — train 0.24572 val 33.62133\n",
      "  Epoch 235/250 — train 0.24430 val 34.15526\n",
      "  Epoch 240/250 — train 0.21989 val 34.02565\n",
      "  Epoch 245/250 — train 0.22620 val 33.05614\n",
      "  Epoch 250/250 — train 0.23113 val 33.43411\n",
      "[TRAIN] ('C', '225_degrees', 'pivot_turn')  (3 files)\n",
      "  Epoch 01/250 — train 1.61186 val 1.86391\n",
      "  Epoch 05/250 — train 0.68381 val 1.62261\n",
      "  Epoch 10/250 — train 0.45550 val 1.86472\n",
      "  Epoch 15/250 — train 0.40980 val 1.64127\n",
      "  Epoch 20/250 — train 0.35316 val 1.74598\n",
      "  Epoch 25/250 — train 0.27191 val 1.65325\n",
      "  Epoch 30/250 — train 0.28748 val 1.44056\n",
      "  Epoch 35/250 — train 0.27451 val 1.37242\n",
      "  Epoch 40/250 — train 0.22150 val 1.45309\n",
      "  Epoch 45/250 — train 0.21886 val 1.47874\n",
      "  Epoch 50/250 — train 0.20257 val 1.47910\n",
      "  Epoch 55/250 — train 0.19931 val 1.43567\n",
      "  Epoch 60/250 — train 0.34795 val 1.96012\n",
      "  Epoch 65/250 — train 0.37933 val 1.63390\n",
      "  Epoch 70/250 — train 0.29339 val 1.69760\n",
      "  Epoch 75/250 — train 0.26870 val 1.40284\n",
      "  Epoch 80/250 — train 0.25788 val 1.50209\n",
      "  Epoch 85/250 — train 0.19225 val 1.48175\n",
      "  Epoch 90/250 — train 0.18263 val 1.47537\n",
      "  Epoch 95/250 — train 0.16775 val 1.53113\n",
      "  Epoch 100/250 — train 0.15214 val 1.43999\n",
      "  Epoch 105/250 — train 0.14810 val 1.47896\n",
      "  Epoch 110/250 — train 0.13654 val 1.52480\n",
      "  Epoch 115/250 — train 0.12462 val 1.57554\n",
      "  Epoch 120/250 — train 0.12149 val 1.52113\n",
      "  Epoch 125/250 — train 0.11602 val 1.54874\n",
      "  Epoch 130/250 — train 0.12111 val 1.40687\n",
      "  Epoch 135/250 — train 0.12298 val 1.54881\n",
      "  Epoch 140/250 — train 0.19058 val 1.58346\n",
      "  Epoch 145/250 — train 0.14003 val 1.50849\n",
      "  Epoch 150/250 — train 0.13255 val 2.60303\n",
      "  Epoch 155/250 — train 0.11893 val 1.78851\n",
      "  Epoch 160/250 — train 0.10728 val 1.49236\n",
      "  Epoch 165/250 — train 0.09688 val 1.52222\n",
      "  Epoch 170/250 — train 0.09590 val 1.74933\n",
      "  Epoch 175/250 — train 0.09210 val 1.88837\n",
      "  Epoch 180/250 — train 0.09137 val 1.53890\n",
      "  Epoch 185/250 — train 0.09477 val 1.51287\n",
      "  Epoch 190/250 — train 0.09796 val 1.47051\n",
      "  Epoch 195/250 — train 0.11470 val 1.56000\n",
      "  Epoch 200/250 — train 0.09228 val 1.50548\n",
      "  Epoch 205/250 — train 0.08512 val 1.46661\n",
      "  Epoch 210/250 — train 0.08883 val 1.47701\n",
      "  Epoch 215/250 — train 0.09010 val 1.92723\n",
      "  Epoch 220/250 — train 0.08704 val 1.59442\n",
      "  Epoch 225/250 — train 0.08796 val 1.86995\n",
      "  Epoch 230/250 — train 0.08514 val 1.97735\n",
      "  Epoch 235/250 — train 0.08104 val 2.28781\n",
      "  Epoch 240/250 — train 0.07773 val 2.13989\n",
      "  Epoch 245/250 — train 0.08863 val 2.72449\n",
      "  Epoch 250/250 — train 0.08640 val 3.04531\n",
      "[TRAIN] ('C', '360_degrees', 'pivot_turn')  (1 files)\n",
      "  Epoch 01/250 — train 1.49587 val nan\n",
      "  Epoch 05/250 — train 1.05250 val nan\n",
      "  Epoch 10/250 — train 0.77786 val nan\n",
      "  Epoch 15/250 — train 0.75633 val nan\n",
      "  Epoch 20/250 — train 0.57603 val nan\n",
      "  Epoch 25/250 — train 0.50181 val nan\n",
      "  Epoch 30/250 — train 0.48165 val nan\n",
      "  Epoch 35/250 — train 0.46928 val nan\n",
      "  Epoch 40/250 — train 0.43864 val nan\n",
      "  Epoch 45/250 — train 0.37357 val nan\n",
      "  Epoch 50/250 — train 0.41547 val nan\n",
      "  Epoch 55/250 — train 0.40827 val nan\n",
      "  Epoch 60/250 — train 0.35693 val nan\n",
      "  Epoch 65/250 — train 0.38912 val nan\n",
      "  Epoch 70/250 — train 0.37172 val nan\n",
      "  Epoch 75/250 — train 0.34461 val nan\n",
      "  Epoch 80/250 — train 0.32262 val nan\n",
      "  Epoch 85/250 — train 0.32609 val nan\n",
      "  Epoch 90/250 — train 0.33346 val nan\n",
      "  Epoch 95/250 — train 0.32075 val nan\n",
      "  Epoch 100/250 — train 0.35814 val nan\n",
      "  Epoch 105/250 — train 0.29407 val nan\n",
      "  Epoch 110/250 — train 0.28957 val nan\n",
      "  Epoch 115/250 — train 0.28662 val nan\n",
      "  Epoch 120/250 — train 0.28509 val nan\n",
      "  Epoch 125/250 — train 0.29695 val nan\n",
      "  Epoch 130/250 — train 0.25488 val nan\n",
      "  Epoch 135/250 — train 0.26317 val nan\n",
      "  Epoch 140/250 — train 0.26210 val nan\n",
      "  Epoch 145/250 — train 0.25856 val nan\n",
      "  Epoch 150/250 — train 0.25717 val nan\n",
      "  Epoch 155/250 — train 0.26768 val nan\n",
      "  Epoch 160/250 — train 0.34948 val nan\n",
      "  Epoch 165/250 — train 0.31441 val nan\n",
      "  Epoch 170/250 — train 0.28095 val nan\n",
      "  Epoch 175/250 — train 0.27456 val nan\n",
      "  Epoch 180/250 — train 0.27261 val nan\n",
      "  Epoch 185/250 — train 0.23907 val nan\n",
      "  Epoch 190/250 — train 0.23136 val nan\n",
      "  Epoch 195/250 — train 0.22464 val nan\n",
      "  Epoch 200/250 — train 0.21330 val nan\n",
      "  Epoch 205/250 — train 0.21323 val nan\n",
      "  Epoch 210/250 — train 0.27050 val nan\n",
      "  Epoch 215/250 — train 0.21964 val nan\n",
      "  Epoch 220/250 — train 0.24469 val nan\n",
      "  Epoch 225/250 — train 0.22748 val nan\n",
      "  Epoch 230/250 — train 0.20868 val nan\n",
      "  Epoch 235/250 — train 0.22721 val nan\n",
      "  Epoch 240/250 — train 0.20238 val nan\n",
      "  Epoch 245/250 — train 0.20344 val nan\n",
      "  Epoch 250/250 — train 0.20702 val nan\n",
      "[TRAIN] ('C', '90_degrees', '-')  (10 files)\n",
      "  Epoch 01/250 — train 1.09203 val 1.57324\n",
      "  Epoch 05/250 — train 0.72751 val 0.82564\n",
      "  Epoch 10/250 — train 0.56865 val 0.83351\n",
      "  Epoch 15/250 — train 0.43387 val 1.40401\n",
      "  Epoch 20/250 — train 0.50000 val 1.07364\n",
      "  Epoch 25/250 — train 0.38332 val 0.74165\n",
      "  Epoch 30/250 — train 0.34484 val 0.92039\n",
      "  Epoch 35/250 — train 0.44255 val 0.84578\n",
      "  Epoch 40/250 — train 0.37145 val 0.79403\n",
      "  Epoch 45/250 — train 0.29514 val 0.91377\n",
      "  Epoch 50/250 — train 0.24230 val 0.93551\n",
      "  Epoch 55/250 — train 0.21377 val 1.57180\n",
      "  Epoch 60/250 — train 0.20646 val 1.26757\n",
      "  Epoch 65/250 — train 0.22657 val 1.35194\n",
      "  Epoch 70/250 — train 0.17627 val 1.38482\n",
      "  Epoch 75/250 — train 0.35292 val 0.77509\n",
      "  Epoch 80/250 — train 0.23259 val 1.37536\n",
      "  Epoch 85/250 — train 0.18846 val 1.21879\n",
      "  Epoch 90/250 — train 0.17101 val 1.44760\n",
      "  Epoch 95/250 — train 0.15689 val 1.52787\n",
      "  Epoch 100/250 — train 0.13790 val 1.34811\n",
      "  Epoch 105/250 — train 0.13324 val 1.35741\n",
      "  Epoch 110/250 — train 0.12641 val 1.44100\n",
      "  Epoch 115/250 — train 0.14891 val 1.29593\n",
      "  Epoch 120/250 — train 0.12915 val 1.44626\n",
      "  Epoch 125/250 — train 0.12015 val 1.03865\n",
      "  Epoch 130/250 — train 0.12084 val 1.12006\n",
      "  Epoch 135/250 — train 0.11449 val 1.12625\n",
      "  Epoch 140/250 — train 0.15523 val 1.24285\n",
      "  Epoch 145/250 — train 0.11851 val 1.13792\n",
      "  Epoch 150/250 — train 0.10731 val 1.09817\n",
      "  Epoch 155/250 — train 0.11049 val 1.36213\n",
      "  Epoch 160/250 — train 0.10996 val 1.09861\n",
      "  Epoch 165/250 — train 0.12629 val 1.02423\n",
      "  Epoch 170/250 — train 0.10298 val 1.11299\n",
      "  Epoch 175/250 — train 0.10183 val 1.03936\n",
      "  Epoch 180/250 — train 0.09816 val 1.08471\n",
      "  Epoch 185/250 — train 0.09737 val 1.08042\n",
      "  Epoch 190/250 — train 0.11017 val 1.09278\n",
      "  Epoch 195/250 — train 0.09602 val 1.00247\n",
      "  Epoch 200/250 — train 0.09177 val 1.01215\n",
      "  Epoch 205/250 — train 0.09424 val 0.93624\n",
      "  Epoch 210/250 — train 0.08687 val 1.00158\n",
      "  Epoch 215/250 — train 0.08841 val 0.96001\n",
      "  Epoch 220/250 — train 0.10481 val 1.07809\n",
      "  Epoch 225/250 — train 0.11041 val 1.24715\n",
      "  Epoch 230/250 — train 0.10856 val 0.99195\n",
      "  Epoch 235/250 — train 0.10382 val 1.04960\n",
      "  Epoch 240/250 — train 0.08348 val 0.98955\n",
      "  Epoch 245/250 — train 0.08259 val 1.01774\n",
      "  Epoch 250/250 — train 0.08041 val 1.01916\n",
      "[TRAIN] ('C', '90_degrees', 'pivot_turn')  (392 files)\n",
      "  Epoch 01/250 — train 0.39904 val 0.27752\n",
      "  Epoch 05/250 — train 0.14582 val 0.16614\n",
      "  Epoch 10/250 — train 0.10288 val 0.12267\n",
      "  Epoch 15/250 — train 0.08587 val 0.11669\n",
      "  Epoch 20/250 — train 0.08181 val 0.10703\n",
      "  Epoch 25/250 — train 0.07821 val 0.10761\n",
      "  Epoch 30/250 — train 0.07577 val 0.10656\n",
      "  Epoch 35/250 — train 0.07501 val 0.11032\n",
      "  Epoch 40/250 — train 0.07267 val 0.11321\n",
      "  Epoch 45/250 — train 0.07097 val 0.11420\n",
      "  Epoch 50/250 — train 0.06918 val 0.11130\n",
      "  Epoch 55/250 — train 0.06796 val 0.11226\n",
      "  Epoch 60/250 — train 0.06763 val 0.11303\n",
      "  Epoch 65/250 — train 0.06639 val 0.11071\n",
      "  Epoch 70/250 — train 0.06566 val 0.11520\n",
      "  Epoch 75/250 — train 0.06456 val 0.11154\n",
      "  Epoch 80/250 — train 0.06382 val 0.11005\n",
      "  Epoch 85/250 — train 0.06353 val 0.11329\n",
      "  Epoch 90/250 — train 0.06307 val 0.11478\n",
      "  Epoch 95/250 — train 0.06268 val 0.11181\n",
      "  Epoch 100/250 — train 0.06213 val 0.11253\n",
      "  Epoch 105/250 — train 0.06182 val 0.11270\n",
      "  Epoch 110/250 — train 0.06148 val 0.11189\n",
      "  Epoch 115/250 — train 0.06277 val 0.11115\n",
      "  Epoch 120/250 — train 0.06092 val 0.11190\n",
      "  Epoch 125/250 — train 0.06050 val 0.11318\n",
      "  Epoch 130/250 — train 0.06080 val 0.11313\n",
      "  Epoch 135/250 — train 0.06045 val 0.11320\n",
      "  Epoch 140/250 — train 0.05974 val 0.11425\n",
      "  Epoch 145/250 — train 0.05932 val 0.11398\n",
      "  Epoch 150/250 — train 0.05986 val 0.11312\n",
      "  Epoch 155/250 — train 0.05917 val 0.11481\n",
      "  Epoch 160/250 — train 0.05936 val 0.11448\n",
      "  Epoch 165/250 — train 0.05931 val 0.11485\n",
      "  Epoch 170/250 — train 0.05891 val 0.11471\n",
      "  Epoch 175/250 — train 0.05860 val 0.11485\n",
      "  Epoch 180/250 — train 0.05860 val 0.11540\n",
      "  Epoch 185/250 — train 0.05837 val 0.11564\n",
      "  Epoch 190/250 — train 0.05835 val 0.11424\n",
      "  Epoch 195/250 — train 0.05824 val 0.11668\n",
      "  Epoch 200/250 — train 0.05811 val 0.11719\n",
      "  Epoch 205/250 — train 0.05809 val 0.11321\n",
      "  Epoch 210/250 — train 0.05782 val 0.11588\n",
      "  Epoch 215/250 — train 0.05803 val 0.11884\n",
      "  Epoch 220/250 — train 0.05793 val 0.11704\n",
      "  Epoch 225/250 — train 0.05766 val 0.11627\n",
      "  Epoch 230/250 — train 0.05768 val 0.11634\n",
      "  Epoch 235/250 — train 0.05717 val 0.11735\n",
      "  Epoch 240/250 — train 0.05731 val 0.11909\n",
      "  Epoch 245/250 — train 0.05709 val 0.11680\n",
      "  Epoch 250/250 — train 0.05689 val 0.12024\n",
      "[TRAIN] ('C', '90_degrees', 'step_turn')  (13 files)\n",
      "  Epoch 01/250 — train 1.15260 val 0.47625\n",
      "  Epoch 05/250 — train 0.64080 val 0.55065\n",
      "  Epoch 10/250 — train 0.44767 val 0.41376\n",
      "  Epoch 15/250 — train 0.41293 val 0.41927\n",
      "  Epoch 20/250 — train 0.36699 val 0.43127\n",
      "  Epoch 25/250 — train 0.33362 val 0.45891\n",
      "  Epoch 30/250 — train 0.32597 val 0.47243\n",
      "  Epoch 35/250 — train 0.28245 val 0.52323\n",
      "  Epoch 40/250 — train 0.30500 val 0.35941\n",
      "  Epoch 45/250 — train 0.28225 val 0.43017\n",
      "  Epoch 50/250 — train 0.24848 val 0.45362\n",
      "  Epoch 55/250 — train 0.23414 val 0.38002\n",
      "  Epoch 60/250 — train 0.22868 val 0.43218\n",
      "  Epoch 65/250 — train 0.22011 val 0.41948\n",
      "  Epoch 70/250 — train 0.24171 val 0.30520\n",
      "  Epoch 75/250 — train 0.19473 val 0.30251\n",
      "  Epoch 80/250 — train 0.18282 val 0.30073\n",
      "  Epoch 85/250 — train 0.20784 val 0.30668\n",
      "  Epoch 90/250 — train 0.17193 val 0.32634\n",
      "  Epoch 95/250 — train 0.14938 val 0.28199\n",
      "  Epoch 100/250 — train 0.13759 val 0.29131\n",
      "  Epoch 105/250 — train 0.12573 val 0.29755\n",
      "  Epoch 110/250 — train 0.14512 val 0.29704\n",
      "  Epoch 115/250 — train 0.12175 val 0.31480\n",
      "  Epoch 120/250 — train 0.10430 val 0.29914\n",
      "  Epoch 125/250 — train 0.10852 val 0.34438\n",
      "  Epoch 130/250 — train 0.16494 val 0.32555\n",
      "  Epoch 135/250 — train 0.12433 val 0.29714\n",
      "  Epoch 140/250 — train 0.10164 val 0.31579\n",
      "  Epoch 145/250 — train 0.11310 val 0.34247\n",
      "  Epoch 150/250 — train 0.11351 val 0.30418\n",
      "  Epoch 155/250 — train 0.10427 val 0.30841\n",
      "  Epoch 160/250 — train 0.10900 val 0.30969\n",
      "  Epoch 165/250 — train 0.14259 val 0.33822\n",
      "  Epoch 170/250 — train 0.08919 val 0.30764\n",
      "  Epoch 175/250 — train 0.16573 val 0.35048\n",
      "  Epoch 180/250 — train 0.09316 val 0.30219\n",
      "  Epoch 185/250 — train 0.10349 val 0.35518\n",
      "  Epoch 190/250 — train 0.10503 val 0.31768\n",
      "  Epoch 195/250 — train 0.08575 val 0.33585\n",
      "  Epoch 200/250 — train 0.10746 val 0.29964\n",
      "  Epoch 205/250 — train 0.08432 val 0.31450\n",
      "  Epoch 210/250 — train 0.08529 val 0.29452\n",
      "  Epoch 215/250 — train 0.09148 val 0.32996\n",
      "  Epoch 220/250 — train 0.11457 val 0.37300\n",
      "  Epoch 225/250 — train 0.14969 val 0.31886\n",
      "  Epoch 230/250 — train 0.11301 val 0.30922\n",
      "  Epoch 235/250 — train 0.14737 val 0.25771\n",
      "  Epoch 240/250 — train 0.09561 val 0.31453\n",
      "  Epoch 245/250 — train 0.09124 val 0.33681\n",
      "  Epoch 250/250 — train 0.08165 val 0.32685\n",
      "[TRAIN] ('PD', '135_degrees', '-')  (4 files)\n",
      "  Epoch 01/250 — train 1.35199 val 20.83350\n",
      "  Epoch 05/250 — train 0.89971 val 20.70641\n",
      "  Epoch 10/250 — train 0.80804 val 18.47279\n",
      "  Epoch 15/250 — train 0.65499 val 18.93463\n",
      "  Epoch 20/250 — train 0.92928 val 19.24601\n",
      "  Epoch 25/250 — train 0.63362 val 19.17701\n",
      "  Epoch 30/250 — train 0.62729 val 18.75832\n",
      "  Epoch 35/250 — train 0.52481 val 18.93278\n",
      "  Epoch 40/250 — train 0.55992 val 18.83958\n",
      "  Epoch 45/250 — train 0.53912 val 19.15094\n",
      "  Epoch 50/250 — train 0.54407 val 18.81887\n",
      "  Epoch 55/250 — train 0.52670 val 18.41674\n",
      "  Epoch 60/250 — train 0.51179 val 18.88533\n",
      "  Epoch 65/250 — train 0.50948 val 18.74635\n",
      "  Epoch 70/250 — train 0.49863 val 18.54668\n",
      "  Epoch 75/250 — train 0.51038 val 18.98203\n",
      "  Epoch 80/250 — train 0.47075 val 18.71638\n",
      "  Epoch 85/250 — train 0.46364 val 18.88823\n",
      "  Epoch 90/250 — train 0.47026 val 18.78276\n",
      "  Epoch 95/250 — train 0.44671 val 18.92648\n",
      "  Epoch 100/250 — train 0.41668 val 18.95808\n",
      "  Epoch 105/250 — train 0.36974 val 18.92338\n",
      "  Epoch 110/250 — train 0.33972 val 18.76245\n",
      "  Epoch 115/250 — train 0.34313 val 18.85118\n",
      "  Epoch 120/250 — train 0.35097 val 18.64879\n",
      "  Epoch 125/250 — train 0.42146 val 19.03057\n",
      "  Epoch 130/250 — train 0.32835 val 18.64070\n",
      "  Epoch 135/250 — train 0.31177 val 18.39897\n",
      "  Epoch 140/250 — train 0.28750 val 18.27539\n",
      "  Epoch 145/250 — train 0.25580 val 17.63956\n",
      "  Epoch 150/250 — train 0.25399 val 18.25734\n",
      "  Epoch 155/250 — train 0.23530 val 18.08989\n",
      "  Epoch 160/250 — train 0.24306 val 17.96545\n",
      "  Epoch 165/250 — train 0.20378 val 17.52196\n",
      "  Epoch 170/250 — train 0.21253 val 17.61255\n",
      "  Epoch 175/250 — train 0.19369 val 17.56462\n",
      "  Epoch 180/250 — train 0.19378 val 17.01582\n",
      "  Epoch 185/250 — train 0.17967 val 17.36940\n",
      "  Epoch 190/250 — train 0.17635 val 17.99200\n",
      "  Epoch 195/250 — train 0.16086 val 17.56378\n",
      "  Epoch 200/250 — train 0.16554 val 17.46723\n",
      "  Epoch 205/250 — train 0.14440 val 17.29494\n",
      "  Epoch 210/250 — train 0.15231 val 17.40892\n",
      "  Epoch 215/250 — train 0.15275 val 17.53256\n",
      "  Epoch 220/250 — train 0.14465 val 17.48256\n",
      "  Epoch 225/250 — train 0.14407 val 17.37317\n",
      "  Epoch 230/250 — train 0.14561 val 17.45066\n",
      "  Epoch 235/250 — train 0.15314 val 17.47975\n",
      "  Epoch 240/250 — train 0.13337 val 17.23068\n",
      "  Epoch 245/250 — train 0.14069 val 17.47251\n",
      "  Epoch 250/250 — train 0.13171 val 17.40979\n",
      "[TRAIN] ('PD', '135_degrees', 'pivot_turn')  (97 files)\n",
      "  Epoch 01/250 — train 0.87775 val 0.65140\n",
      "  Epoch 05/250 — train 0.22415 val 0.25854\n",
      "  Epoch 10/250 — train 0.19021 val 0.22848\n",
      "  Epoch 15/250 — train 0.17554 val 0.23659\n",
      "  Epoch 20/250 — train 0.16423 val 0.24183\n",
      "  Epoch 25/250 — train 0.15096 val 0.22756\n",
      "  Epoch 30/250 — train 0.16523 val 0.21144\n",
      "  Epoch 35/250 — train 0.14872 val 0.23633\n",
      "  Epoch 40/250 — train 0.14116 val 0.22463\n",
      "  Epoch 45/250 — train 0.14095 val 0.22811\n",
      "  Epoch 50/250 — train 0.13699 val 0.19702\n",
      "  Epoch 55/250 — train 0.11862 val 0.18475\n",
      "  Epoch 60/250 — train 0.10299 val 0.17612\n",
      "  Epoch 65/250 — train 0.10519 val 0.18305\n",
      "  Epoch 70/250 — train 0.09220 val 0.20618\n",
      "  Epoch 75/250 — train 0.09292 val 0.18071\n",
      "  Epoch 80/250 — train 0.09998 val 0.20368\n",
      "  Epoch 85/250 — train 0.08950 val 0.19569\n",
      "  Epoch 90/250 — train 0.08784 val 0.19766\n",
      "  Epoch 95/250 — train 0.08481 val 0.19297\n",
      "  Epoch 100/250 — train 0.09269 val 0.19926\n",
      "  Epoch 105/250 — train 0.08273 val 0.19424\n",
      "  Epoch 110/250 — train 0.08499 val 0.20395\n",
      "  Epoch 115/250 — train 0.07914 val 0.19367\n",
      "  Epoch 120/250 — train 0.07998 val 0.19758\n",
      "  Epoch 125/250 — train 0.07896 val 0.21128\n",
      "  Epoch 130/250 — train 0.07554 val 0.20173\n",
      "  Epoch 135/250 — train 0.07367 val 0.21653\n",
      "  Epoch 140/250 — train 0.07779 val 0.20578\n",
      "  Epoch 145/250 — train 0.07390 val 0.22189\n",
      "  Epoch 150/250 — train 0.07131 val 0.19315\n",
      "  Epoch 155/250 — train 0.07306 val 0.21970\n",
      "  Epoch 160/250 — train 0.07096 val 0.20624\n",
      "  Epoch 165/250 — train 0.07121 val 0.21278\n",
      "  Epoch 170/250 — train 0.06837 val 0.22196\n",
      "  Epoch 175/250 — train 0.06824 val 0.20908\n",
      "  Epoch 180/250 — train 0.06663 val 0.22806\n",
      "  Epoch 185/250 — train 0.06556 val 0.22413\n",
      "  Epoch 190/250 — train 0.07467 val 0.21171\n",
      "  Epoch 195/250 — train 0.07316 val 0.22314\n",
      "  Epoch 200/250 — train 0.06504 val 0.22383\n",
      "  Epoch 205/250 — train 0.06533 val 0.22223\n",
      "  Epoch 210/250 — train 0.06463 val 0.23209\n",
      "  Epoch 215/250 — train 0.07939 val 0.25708\n",
      "  Epoch 220/250 — train 0.06396 val 0.21452\n",
      "  Epoch 225/250 — train 0.07144 val 0.22909\n",
      "  Epoch 230/250 — train 0.06337 val 0.22633\n",
      "  Epoch 235/250 — train 0.06723 val 0.22426\n",
      "  Epoch 240/250 — train 0.06121 val 0.23298\n",
      "  Epoch 245/250 — train 0.06275 val 0.23108\n",
      "  Epoch 250/250 — train 0.06307 val 0.22706\n",
      "[TRAIN] ('PD', '135_degrees', 'step_turn')  (33 files)\n",
      "  Epoch 01/250 — train 0.83396 val 0.47469\n",
      "  Epoch 05/250 — train 0.50225 val 0.54775\n",
      "  Epoch 10/250 — train 0.27096 val 0.32262\n",
      "  Epoch 15/250 — train 0.24612 val 0.26380\n",
      "  Epoch 20/250 — train 0.18316 val 0.27264\n",
      "  Epoch 25/250 — train 0.17589 val 0.26670\n",
      "  Epoch 30/250 — train 0.19864 val 0.23632\n",
      "  Epoch 35/250 — train 0.19615 val 0.26316\n",
      "  Epoch 40/250 — train 0.16685 val 0.28004\n",
      "  Epoch 45/250 — train 0.12649 val 0.24954\n",
      "  Epoch 50/250 — train 0.14038 val 0.26775\n",
      "  Epoch 55/250 — train 0.12717 val 0.29313\n",
      "  Epoch 60/250 — train 0.11485 val 0.25962\n",
      "  Epoch 65/250 — train 0.11506 val 0.27441\n",
      "  Epoch 70/250 — train 0.11718 val 0.29652\n",
      "  Epoch 75/250 — train 0.10870 val 0.28722\n",
      "  Epoch 80/250 — train 0.11069 val 0.27978\n",
      "  Epoch 85/250 — train 0.10004 val 0.30055\n",
      "  Epoch 90/250 — train 0.15185 val 0.26035\n",
      "  Epoch 95/250 — train 0.09713 val 0.29211\n",
      "  Epoch 100/250 — train 0.13848 val 0.26936\n",
      "  Epoch 105/250 — train 0.11707 val 0.27380\n",
      "  Epoch 110/250 — train 0.11150 val 0.27881\n",
      "  Epoch 115/250 — train 0.11706 val 0.28003\n",
      "  Epoch 120/250 — train 0.10298 val 0.29261\n",
      "  Epoch 125/250 — train 0.09042 val 0.27977\n",
      "  Epoch 130/250 — train 0.20199 val 0.27956\n",
      "  Epoch 135/250 — train 0.09376 val 0.28899\n",
      "  Epoch 140/250 — train 0.08608 val 0.29614\n",
      "  Epoch 145/250 — train 0.08604 val 0.28365\n",
      "  Epoch 150/250 — train 0.08995 val 0.28414\n",
      "  Epoch 155/250 — train 0.08915 val 0.28398\n",
      "  Epoch 160/250 — train 0.09215 val 0.27414\n",
      "  Epoch 165/250 — train 0.13720 val 0.31684\n",
      "  Epoch 170/250 — train 0.11017 val 0.30619\n",
      "  Epoch 175/250 — train 0.09179 val 0.29493\n",
      "  Epoch 180/250 — train 0.08934 val 0.34966\n",
      "  Epoch 185/250 — train 0.10565 val 0.29770\n",
      "  Epoch 190/250 — train 0.09548 val 0.31050\n",
      "  Epoch 195/250 — train 0.10137 val 0.30663\n",
      "  Epoch 200/250 — train 0.16133 val 0.32741\n",
      "  Epoch 205/250 — train 0.08841 val 0.30263\n",
      "  Epoch 210/250 — train 0.08679 val 0.32701\n",
      "  Epoch 215/250 — train 0.11067 val 0.30821\n",
      "  Epoch 220/250 — train 0.07838 val 0.34564\n",
      "  Epoch 225/250 — train 0.08694 val 0.30230\n",
      "  Epoch 230/250 — train 0.07976 val 0.27610\n",
      "  Epoch 235/250 — train 0.11335 val 0.30964\n",
      "  Epoch 240/250 — train 0.07954 val 0.30905\n",
      "  Epoch 245/250 — train 0.08348 val 0.29122\n",
      "  Epoch 250/250 — train 0.07258 val 0.29639\n",
      "[TRAIN] ('PD', '180_degrees', '-')  (4 files)\n",
      "  Epoch 01/250 — train 1.47205 val 1.54902\n",
      "  Epoch 05/250 — train 1.01996 val 1.46285\n",
      "  Epoch 10/250 — train 0.87978 val 1.27194\n",
      "  Epoch 15/250 — train 0.55819 val 1.18930\n",
      "  Epoch 20/250 — train 0.46868 val 1.18272\n",
      "  Epoch 25/250 — train 0.44976 val 1.12341\n",
      "  Epoch 30/250 — train 0.42795 val 1.12306\n",
      "  Epoch 35/250 — train 0.38491 val 1.11043\n",
      "  Epoch 40/250 — train 0.38374 val 1.02828\n",
      "  Epoch 45/250 — train 0.37153 val 0.96239\n",
      "  Epoch 50/250 — train 0.35739 val 0.94735\n",
      "  Epoch 55/250 — train 0.35935 val 0.96300\n",
      "  Epoch 60/250 — train 0.33749 val 0.86535\n",
      "  Epoch 65/250 — train 0.35503 val 0.92466\n",
      "  Epoch 70/250 — train 0.35399 val 0.85852\n",
      "  Epoch 75/250 — train 0.31954 val 0.92533\n",
      "  Epoch 80/250 — train 0.30743 val 0.88422\n",
      "  Epoch 85/250 — train 0.31241 val 0.84499\n",
      "  Epoch 90/250 — train 0.30271 val 0.80338\n",
      "  Epoch 95/250 — train 0.29085 val 0.78921\n",
      "  Epoch 100/250 — train 0.32623 val 0.83846\n",
      "  Epoch 105/250 — train 0.31794 val 0.76780\n",
      "  Epoch 110/250 — train 0.28990 val 0.96889\n",
      "  Epoch 115/250 — train 0.28021 val 0.85636\n",
      "  Epoch 120/250 — train 0.30712 val 0.73175\n",
      "  Epoch 125/250 — train 0.29161 val 0.84624\n",
      "  Epoch 130/250 — train 0.28770 val 0.75126\n",
      "  Epoch 135/250 — train 0.27118 val 0.75092\n",
      "  Epoch 140/250 — train 0.28628 val 0.82684\n",
      "  Epoch 145/250 — train 0.29508 val 0.76957\n",
      "  Epoch 150/250 — train 0.29363 val 0.76911\n",
      "  Epoch 155/250 — train 0.23999 val 0.68775\n",
      "  Epoch 160/250 — train 0.22573 val 0.68257\n",
      "  Epoch 165/250 — train 0.21565 val 0.69497\n",
      "  Epoch 170/250 — train 0.20643 val 0.70913\n",
      "  Epoch 175/250 — train 0.21047 val 0.73048\n",
      "  Epoch 180/250 — train 0.21520 val 0.68994\n",
      "  Epoch 185/250 — train 0.21317 val 0.75350\n",
      "  Epoch 190/250 — train 0.21082 val 0.69706\n",
      "  Epoch 195/250 — train 0.19724 val 0.70703\n",
      "  Epoch 200/250 — train 0.20502 val 0.72214\n",
      "  Epoch 205/250 — train 0.20626 val 0.72034\n",
      "  Epoch 210/250 — train 0.21017 val 0.69778\n",
      "  Epoch 215/250 — train 0.20068 val 0.70445\n",
      "  Epoch 220/250 — train 0.19703 val 0.68589\n",
      "  Epoch 225/250 — train 0.20215 val 0.71317\n",
      "  Epoch 230/250 — train 0.22030 val 0.72909\n",
      "  Epoch 235/250 — train 0.21743 val 0.73873\n",
      "  Epoch 240/250 — train 0.20505 val 0.69830\n",
      "  Epoch 245/250 — train 0.19833 val 0.71553\n",
      "  Epoch 250/250 — train 0.19221 val 0.69671\n",
      "[TRAIN] ('PD', '180_degrees', 'pivot_turn')  (209 files)\n",
      "  Epoch 01/250 — train 0.73009 val 0.44863\n",
      "  Epoch 05/250 — train 0.76274 val 0.51885\n",
      "  Epoch 10/250 — train 0.71599 val 0.39832\n",
      "  Epoch 15/250 — train 0.53261 val 0.35504\n",
      "  Epoch 20/250 — train 0.50146 val 0.40748\n",
      "  Epoch 25/250 — train 0.40096 val 0.32562\n",
      "  Epoch 30/250 — train 0.32431 val 0.24150\n",
      "  Epoch 35/250 — train 0.22915 val 0.20748\n",
      "  Epoch 40/250 — train 0.22282 val 0.21249\n",
      "  Epoch 45/250 — train 0.21201 val 0.21009\n",
      "  Epoch 50/250 — train 0.21304 val 0.20792\n",
      "  Epoch 55/250 — train 0.20891 val 0.20806\n",
      "  Epoch 60/250 — train 0.19864 val 0.20273\n",
      "  Epoch 65/250 — train 0.19023 val 0.18962\n",
      "  Epoch 70/250 — train 0.18198 val 0.18853\n",
      "  Epoch 75/250 — train 0.18695 val 0.18415\n",
      "  Epoch 80/250 — train 0.17895 val 0.19465\n",
      "  Epoch 85/250 — train 0.17564 val 0.18842\n",
      "  Epoch 90/250 — train 0.17031 val 0.18562\n",
      "  Epoch 95/250 — train 0.17809 val 0.18364\n",
      "  Epoch 100/250 — train 0.16534 val 0.18476\n",
      "  Epoch 105/250 — train 0.16162 val 0.18807\n",
      "  Epoch 110/250 — train 0.16531 val 0.18632\n",
      "  Epoch 115/250 — train 0.16433 val 0.19199\n",
      "  Epoch 120/250 — train 0.16533 val 0.18180\n",
      "  Epoch 125/250 — train 0.15893 val 0.18002\n",
      "  Epoch 130/250 — train 0.15414 val 0.17444\n",
      "  Epoch 135/250 — train 0.15596 val 0.20179\n",
      "  Epoch 140/250 — train 0.15351 val 0.18173\n",
      "  Epoch 145/250 — train 0.15204 val 0.18222\n",
      "  Epoch 150/250 — train 0.15310 val 0.18113\n",
      "  Epoch 155/250 — train 0.15506 val 0.17026\n",
      "  Epoch 160/250 — train 0.15069 val 0.18144\n",
      "  Epoch 165/250 — train 0.15704 val 0.18331\n",
      "  Epoch 170/250 — train 0.15339 val 0.16756\n",
      "  Epoch 175/250 — train 0.14983 val 0.17824\n",
      "  Epoch 180/250 — train 0.15029 val 0.17898\n",
      "  Epoch 185/250 — train 0.14654 val 0.18245\n",
      "  Epoch 190/250 — train 0.15516 val 0.17464\n",
      "  Epoch 195/250 — train 0.14505 val 0.16671\n",
      "  Epoch 200/250 — train 0.15154 val 0.17811\n",
      "  Epoch 205/250 — train 0.14999 val 0.17915\n",
      "  Epoch 210/250 — train 0.14195 val 0.19260\n",
      "  Epoch 215/250 — train 0.14654 val 0.17569\n",
      "  Epoch 220/250 — train 0.15654 val 0.20844\n",
      "  Epoch 225/250 — train 0.21382 val 0.23526\n",
      "  Epoch 230/250 — train 0.19818 val 0.20477\n",
      "  Epoch 235/250 — train 0.15578 val 0.19113\n",
      "  Epoch 240/250 — train 0.15052 val 0.18218\n",
      "  Epoch 245/250 — train 0.14453 val 0.18602\n",
      "  Epoch 250/250 — train 0.14672 val 0.17909\n",
      "[TRAIN] ('PD', '180_degrees', 'step_turn')  (76 files)\n",
      "  Epoch 01/250 — train 0.74660 val 0.75235\n",
      "  Epoch 05/250 — train 0.68376 val 1.34584\n",
      "  Epoch 10/250 — train 0.51240 val 1.24807\n",
      "  Epoch 15/250 — train 0.57634 val 1.06295\n",
      "  Epoch 20/250 — train 0.94792 val 2.21172\n",
      "  Epoch 25/250 — train 0.94407 val 1.62171\n",
      "  Epoch 30/250 — train 0.86612 val 2.33916\n",
      "  Epoch 35/250 — train 0.84044 val 1.47307\n",
      "  Epoch 40/250 — train 0.83959 val 1.47615\n",
      "  Epoch 45/250 — train 0.83642 val 1.49192\n",
      "  Epoch 50/250 — train 0.87837 val 1.59718\n",
      "  Epoch 55/250 — train 0.81593 val 1.50450\n",
      "  Epoch 60/250 — train 0.82827 val 1.54037\n",
      "  Epoch 65/250 — train 0.98050 val 1.72436\n",
      "  Epoch 70/250 — train 0.97209 val 1.75090\n",
      "  Epoch 75/250 — train 0.98121 val 1.71023\n",
      "  Epoch 80/250 — train 0.97979 val 1.70501\n",
      "  Epoch 85/250 — train 0.97939 val 1.71840\n",
      "  Epoch 90/250 — train 0.98266 val 1.70718\n",
      "  Epoch 95/250 — train 0.97983 val 1.72898\n",
      "  Epoch 100/250 — train 0.97840 val 1.73607\n",
      "  Epoch 105/250 — train 0.97968 val 1.72004\n",
      "  Epoch 110/250 — train 0.97790 val 1.69947\n",
      "  Epoch 115/250 — train 0.97358 val 1.71052\n",
      "  Epoch 120/250 — train 0.97687 val 1.71689\n",
      "  Epoch 125/250 — train 0.97646 val 1.71059\n",
      "  Epoch 130/250 — train 0.97881 val 1.71819\n",
      "  Epoch 135/250 — train 0.97655 val 1.71173\n",
      "  Epoch 140/250 — train 0.97818 val 1.70807\n",
      "  Epoch 145/250 — train 0.97779 val 1.71519\n",
      "  Epoch 150/250 — train 0.97759 val 1.72392\n",
      "  Epoch 155/250 — train 0.97758 val 1.71886\n",
      "  Epoch 160/250 — train 0.97741 val 1.72756\n",
      "  Epoch 165/250 — train 0.97683 val 1.71588\n",
      "  Epoch 170/250 — train 0.97813 val 1.71372\n",
      "  Epoch 175/250 — train 0.98212 val 1.71429\n",
      "  Epoch 180/250 — train 0.98113 val 1.71688\n",
      "  Epoch 185/250 — train 0.98170 val 1.71631\n",
      "  Epoch 190/250 — train 0.97666 val 1.72093\n",
      "  Epoch 195/250 — train 0.97865 val 1.71401\n",
      "  Epoch 200/250 — train 0.97975 val 1.71622\n",
      "  Epoch 205/250 — train 0.98196 val 1.71487\n",
      "  Epoch 210/250 — train 0.97315 val 1.71962\n",
      "  Epoch 215/250 — train 0.98028 val 1.71632\n",
      "  Epoch 220/250 — train 0.97625 val 1.72627\n",
      "  Epoch 225/250 — train 0.97601 val 1.71209\n",
      "  Epoch 230/250 — train 0.97662 val 1.71586\n",
      "  Epoch 235/250 — train 0.97609 val 1.72241\n",
      "  Epoch 240/250 — train 0.97621 val 1.71559\n",
      "  Epoch 245/250 — train 0.97064 val 1.71635\n",
      "  Epoch 250/250 — train 0.97738 val 1.71683\n",
      "[TRAIN] ('PD', '225_degrees', 'step_turn')  (2 files)\n",
      "  Epoch 01/250 — train 1.34326 val 73.98481\n",
      "  Epoch 05/250 — train 1.03760 val 71.88155\n",
      "  Epoch 10/250 — train 0.79530 val 70.31286\n",
      "  Epoch 15/250 — train 0.67998 val 69.32387\n",
      "  Epoch 20/250 — train 0.65509 val 68.06140\n",
      "  Epoch 25/250 — train 0.63425 val 69.47186\n",
      "  Epoch 30/250 — train 0.60098 val 68.30184\n",
      "  Epoch 35/250 — train 0.64730 val 68.59576\n",
      "  Epoch 40/250 — train 0.62938 val 68.94152\n",
      "  Epoch 45/250 — train 0.61295 val 68.15107\n",
      "  Epoch 50/250 — train 0.62672 val 69.96330\n",
      "  Epoch 55/250 — train 0.61316 val 69.21249\n",
      "  Epoch 60/250 — train 0.61322 val 68.82224\n",
      "  Epoch 65/250 — train 0.61674 val 70.00744\n",
      "  Epoch 70/250 — train 0.58414 val 69.26169\n",
      "  Epoch 75/250 — train 0.60242 val 69.68495\n",
      "  Epoch 80/250 — train 0.60451 val 69.33211\n",
      "  Epoch 85/250 — train 0.51225 val 67.57768\n",
      "  Epoch 90/250 — train 0.44782 val 65.21401\n",
      "  Epoch 95/250 — train 0.42372 val 64.39337\n",
      "  Epoch 100/250 — train 0.40986 val 64.65148\n",
      "  Epoch 105/250 — train 0.34559 val 63.45939\n",
      "  Epoch 110/250 — train 0.31930 val 63.68332\n",
      "  Epoch 115/250 — train 0.30111 val 63.70077\n",
      "  Epoch 120/250 — train 0.27101 val 64.36516\n",
      "  Epoch 125/250 — train 0.31647 val 65.54803\n",
      "  Epoch 130/250 — train 0.37215 val 64.21418\n",
      "  Epoch 135/250 — train 0.27896 val 64.35681\n",
      "  Epoch 140/250 — train 0.35028 val 66.63379\n",
      "  Epoch 145/250 — train 0.26315 val 64.16456\n",
      "  Epoch 150/250 — train 0.23916 val 64.18622\n",
      "  Epoch 155/250 — train 0.24755 val 63.52695\n",
      "  Epoch 160/250 — train 0.25051 val 63.15045\n",
      "  Epoch 165/250 — train 0.23239 val 64.04214\n",
      "  Epoch 170/250 — train 0.22371 val 63.71532\n",
      "  Epoch 175/250 — train 0.20308 val 62.94157\n",
      "  Epoch 180/250 — train 0.21688 val 62.59146\n",
      "  Epoch 185/250 — train 0.20175 val 63.17050\n",
      "  Epoch 190/250 — train 0.21739 val 63.46535\n",
      "  Epoch 195/250 — train 0.22212 val 63.11630\n",
      "  Epoch 200/250 — train 0.20920 val 63.59306\n",
      "  Epoch 205/250 — train 0.19502 val 63.37463\n",
      "  Epoch 210/250 — train 0.18941 val 63.22300\n",
      "  Epoch 215/250 — train 0.18463 val 63.23134\n",
      "  Epoch 220/250 — train 0.18183 val 63.11334\n",
      "  Epoch 225/250 — train 0.18123 val 63.09359\n",
      "  Epoch 230/250 — train 0.19797 val 63.10272\n",
      "  Epoch 235/250 — train 0.18114 val 63.71386\n",
      "  Epoch 240/250 — train 0.17390 val 63.45858\n",
      "  Epoch 245/250 — train 0.18460 val 63.15238\n",
      "  Epoch 250/250 — train 0.19789 val 63.15168\n",
      "[TRAIN] ('PD', '270_degrees', 'pivot_turn')  (1 files)\n",
      "  [SKIP small] ('PD', '270_degrees', 'pivot_turn') — not enough windows\n",
      "[TRAIN] ('PD', '90_degrees', '-')  (18 files)\n",
      "  Epoch 01/250 — train 0.98456 val 0.67408\n",
      "  Epoch 05/250 — train 0.49157 val 0.52464\n",
      "  Epoch 10/250 — train 0.44568 val 0.41406\n",
      "  Epoch 15/250 — train 0.40406 val 0.54226\n",
      "  Epoch 20/250 — train 0.29387 val 0.40183\n",
      "  Epoch 25/250 — train 0.24372 val 0.55547\n",
      "  Epoch 30/250 — train 0.20981 val 0.54010\n",
      "  Epoch 35/250 — train 0.17104 val 0.57631\n",
      "  Epoch 40/250 — train 0.15756 val 0.42175\n",
      "  Epoch 45/250 — train 0.15189 val 0.46034\n",
      "  Epoch 50/250 — train 0.13048 val 0.41659\n",
      "  Epoch 55/250 — train 0.13135 val 0.42614\n",
      "  Epoch 60/250 — train 0.11785 val 0.44614\n",
      "  Epoch 65/250 — train 0.14877 val 0.45078\n",
      "  Epoch 70/250 — train 0.10064 val 0.44721\n",
      "  Epoch 75/250 — train 0.10414 val 0.42775\n",
      "  Epoch 80/250 — train 0.10534 val 0.48475\n",
      "  Epoch 85/250 — train 0.14910 val 0.47873\n",
      "  Epoch 90/250 — train 0.15800 val 0.41766\n",
      "  Epoch 95/250 — train 0.10957 val 0.49737\n",
      "  Epoch 100/250 — train 0.10088 val 0.52568\n",
      "  Epoch 105/250 — train 0.08857 val 0.51488\n",
      "  Epoch 110/250 — train 0.10734 val 0.47663\n",
      "  Epoch 115/250 — train 0.10622 val 0.61531\n",
      "  Epoch 120/250 — train 0.09187 val 0.53417\n",
      "  Epoch 125/250 — train 0.09443 val 0.54348\n",
      "  Epoch 130/250 — train 0.08093 val 0.54026\n",
      "  Epoch 135/250 — train 0.08098 val 0.53385\n",
      "  Epoch 140/250 — train 0.07427 val 0.53652\n",
      "  Epoch 145/250 — train 0.10489 val 0.59091\n",
      "  Epoch 150/250 — train 0.09831 val 0.52485\n",
      "  Epoch 155/250 — train 0.08997 val 0.48913\n",
      "  Epoch 160/250 — train 0.08920 val 0.49361\n",
      "  Epoch 165/250 — train 0.15083 val 0.63346\n",
      "  Epoch 170/250 — train 0.13891 val 0.55851\n",
      "  Epoch 175/250 — train 0.07013 val 0.56191\n",
      "  Epoch 180/250 — train 0.07265 val 0.50715\n",
      "  Epoch 185/250 — train 0.07809 val 0.50371\n",
      "  Epoch 190/250 — train 0.06614 val 0.53774\n",
      "  Epoch 195/250 — train 0.07395 val 0.56421\n",
      "  Epoch 200/250 — train 0.06453 val 0.55948\n",
      "  Epoch 205/250 — train 0.06442 val 0.56058\n",
      "  Epoch 210/250 — train 0.06966 val 0.52873\n",
      "  Epoch 215/250 — train 0.06855 val 0.53797\n",
      "  Epoch 220/250 — train 0.06020 val 0.57511\n",
      "  Epoch 225/250 — train 0.06003 val 0.60492\n",
      "  Epoch 230/250 — train 0.05980 val 0.56747\n",
      "  Epoch 235/250 — train 0.06184 val 0.60083\n",
      "  Epoch 240/250 — train 0.06011 val 0.57923\n",
      "  Epoch 245/250 — train 0.06465 val 0.59043\n",
      "  Epoch 250/250 — train 0.06860 val 0.55609\n",
      "[TRAIN] ('PD', '90_degrees', 'pivot_turn')  (404 files)\n",
      "  Epoch 01/250 — train 0.52004 val 0.40750\n",
      "  Epoch 05/250 — train 0.21217 val 0.26074\n",
      "  Epoch 10/250 — train 0.15172 val 0.21803\n",
      "  Epoch 15/250 — train 0.12768 val 0.15405\n",
      "  Epoch 20/250 — train 0.14838 val 0.15147\n",
      "  Epoch 25/250 — train 0.11760 val 0.15917\n",
      "  Epoch 30/250 — train 0.10481 val 0.14979\n",
      "  Epoch 35/250 — train 0.09817 val 0.16574\n",
      "  Epoch 40/250 — train 0.09447 val 0.15074\n",
      "  Epoch 45/250 — train 0.10021 val 0.15397\n",
      "  Epoch 50/250 — train 0.08454 val 0.15944\n",
      "  Epoch 55/250 — train 0.08328 val 0.15196\n",
      "  Epoch 60/250 — train 0.08163 val 0.14266\n",
      "  Epoch 65/250 — train 0.07992 val 0.13585\n",
      "  Epoch 70/250 — train 0.07860 val 0.14426\n",
      "  Epoch 75/250 — train 0.07745 val 0.15397\n",
      "  Epoch 80/250 — train 0.07717 val 0.13751\n",
      "  Epoch 85/250 — train 0.07777 val 0.14199\n",
      "  Epoch 90/250 — train 0.07524 val 0.14351\n",
      "  Epoch 95/250 — train 0.07444 val 0.16176\n",
      "  Epoch 100/250 — train 0.09441 val 0.15188\n",
      "  Epoch 105/250 — train 0.07297 val 0.15262\n",
      "  Epoch 110/250 — train 0.08917 val 0.13625\n",
      "  Epoch 115/250 — train 0.07199 val 0.15036\n",
      "  Epoch 120/250 — train 0.07193 val 0.14988\n",
      "  Epoch 125/250 — train 0.07191 val 0.15323\n",
      "  Epoch 130/250 — train 0.07130 val 0.14380\n",
      "  Epoch 135/250 — train 0.07075 val 0.14562\n",
      "  Epoch 140/250 — train 0.07069 val 0.14674\n",
      "  Epoch 145/250 — train 0.07000 val 0.14667\n",
      "  Epoch 150/250 — train 0.07005 val 0.14858\n",
      "  Epoch 155/250 — train 0.06818 val 0.14503\n",
      "  Epoch 160/250 — train 0.06857 val 0.14722\n",
      "  Epoch 165/250 — train 0.06811 val 0.16784\n",
      "  Epoch 170/250 — train 0.06774 val 0.14966\n",
      "  Epoch 175/250 — train 0.06802 val 0.16545\n",
      "  Epoch 180/250 — train 0.06701 val 0.16255\n",
      "  Epoch 185/250 — train 0.06710 val 0.14895\n",
      "  Epoch 190/250 — train 0.06711 val 0.16299\n",
      "  Epoch 195/250 — train 0.06683 val 0.18619\n",
      "  Epoch 200/250 — train 0.06687 val 0.16183\n",
      "  Epoch 205/250 — train 0.06710 val 0.19374\n",
      "  Epoch 210/250 — train 0.06638 val 0.18542\n",
      "  Epoch 215/250 — train 0.06663 val 0.17305\n",
      "  Epoch 220/250 — train 0.06591 val 0.14709\n",
      "  Epoch 225/250 — train 0.06582 val 0.15470\n",
      "  Epoch 230/250 — train 0.06585 val 0.17521\n",
      "  Epoch 235/250 — train 0.07169 val 0.15411\n",
      "  Epoch 240/250 — train 0.06490 val 0.19406\n",
      "  Epoch 245/250 — train 0.06550 val 0.16138\n",
      "  Epoch 250/250 — train 0.06545 val 0.16646\n",
      "[TRAIN] ('PD', '90_degrees', 'step_turn')  (90 files)\n",
      "  Epoch 01/250 — train 0.74986 val 0.55384\n",
      "  Epoch 05/250 — train 0.28685 val 0.48599\n",
      "  Epoch 10/250 — train 0.16485 val 0.42507\n",
      "  Epoch 15/250 — train 0.14450 val 0.44827\n",
      "  Epoch 20/250 — train 0.12634 val 0.39972\n",
      "  Epoch 25/250 — train 0.11931 val 0.47668\n",
      "  Epoch 30/250 — train 0.10761 val 0.44778\n",
      "  Epoch 35/250 — train 0.10686 val 0.45749\n",
      "  Epoch 40/250 — train 0.09895 val 0.44457\n",
      "  Epoch 45/250 — train 0.09430 val 0.44500\n",
      "  Epoch 50/250 — train 0.10332 val 0.47776\n",
      "  Epoch 55/250 — train 0.08769 val 0.47162\n",
      "  Epoch 60/250 — train 0.08108 val 0.45582\n",
      "  Epoch 65/250 — train 0.08395 val 0.56493\n",
      "  Epoch 70/250 — train 0.08051 val 0.48533\n",
      "  Epoch 75/250 — train 0.08149 val 0.49691\n",
      "  Epoch 80/250 — train 0.08196 val 0.47048\n",
      "  Epoch 85/250 — train 0.13267 val 0.50951\n",
      "  Epoch 90/250 — train 0.07525 val 0.49431\n",
      "  Epoch 95/250 — train 0.07574 val 0.49631\n",
      "  Epoch 100/250 — train 0.07655 val 0.51669\n",
      "  Epoch 105/250 — train 0.07604 val 0.48502\n",
      "  Epoch 110/250 — train 0.07320 val 0.48462\n",
      "  Epoch 115/250 — train 0.06962 val 0.49043\n",
      "  Epoch 120/250 — train 0.07195 val 0.53094\n",
      "  Epoch 125/250 — train 0.07580 val 0.51716\n",
      "  Epoch 130/250 — train 0.07330 val 0.55924\n",
      "  Epoch 135/250 — train 0.06958 val 0.50555\n",
      "  Epoch 140/250 — train 0.08459 val 0.54864\n",
      "  Epoch 145/250 — train 0.06858 val 0.55649\n",
      "  Epoch 150/250 — train 0.06771 val 0.54511\n",
      "  Epoch 155/250 — train 0.07100 val 0.53894\n",
      "  Epoch 160/250 — train 0.10848 val 0.59421\n",
      "  Epoch 165/250 — train 0.06717 val 0.55340\n",
      "  Epoch 170/250 — train 0.06440 val 0.54897\n",
      "  Epoch 175/250 — train 0.06867 val 0.59070\n",
      "  Epoch 180/250 — train 0.06412 val 0.52289\n",
      "  Epoch 185/250 — train 0.06688 val 0.55088\n",
      "  Epoch 190/250 — train 0.06347 val 0.55013\n",
      "  Epoch 195/250 — train 0.06368 val 0.52004\n",
      "  Epoch 200/250 — train 0.06146 val 0.52614\n",
      "  Epoch 205/250 — train 0.06585 val 0.52215\n",
      "  Epoch 210/250 — train 0.06489 val 0.58180\n",
      "  Epoch 215/250 — train 0.06298 val 0.51225\n",
      "  Epoch 220/250 — train 0.06333 val 0.54638\n",
      "  Epoch 225/250 — train 0.06992 val 0.56193\n",
      "  Epoch 230/250 — train 0.06797 val 0.58798\n",
      "  Epoch 235/250 — train 0.06551 val 0.55022\n",
      "  Epoch 240/250 — train 0.10232 val 0.54424\n",
      "  Epoch 245/250 — train 0.06090 val 0.54698\n",
      "  Epoch 250/250 — train 0.06051 val 0.53062\n",
      "\n",
      "Saved model manifest: D:\\Courses\\thesis\\data\\second\\models_turning_norm_250\\_models_manifest.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   PD_or_C turning_angle type_of_turn  n_files  model_saved  \\\n",
       "0        C   135_degrees            -        1        False   \n",
       "1        C   135_degrees   pivot_turn       74         True   \n",
       "2        C   135_degrees    step_turn        3         True   \n",
       "3        C   180_degrees   pivot_turn      234         True   \n",
       "4        C   180_degrees    step_turn       12         True   \n",
       "5        C   225_degrees   pivot_turn        3         True   \n",
       "6        C   360_degrees   pivot_turn        1         True   \n",
       "7        C    90_degrees            -       10         True   \n",
       "8        C    90_degrees   pivot_turn      392         True   \n",
       "9        C    90_degrees    step_turn       13         True   \n",
       "10      PD   135_degrees            -        4         True   \n",
       "11      PD   135_degrees   pivot_turn       97         True   \n",
       "12      PD   135_degrees    step_turn       33         True   \n",
       "13      PD   180_degrees            -        4         True   \n",
       "14      PD   180_degrees   pivot_turn      209         True   \n",
       "15      PD   180_degrees    step_turn       76         True   \n",
       "16      PD   225_degrees    step_turn        2         True   \n",
       "17      PD   270_degrees   pivot_turn        1        False   \n",
       "18      PD    90_degrees            -       18         True   \n",
       "19      PD    90_degrees   pivot_turn      404         True   \n",
       "\n",
       "                                           model_path  \n",
       "0                                                None  \n",
       "1   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "2   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "3   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "4   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "5   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "6   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "7   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "8   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "9   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "10  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "11  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "12  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "13  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "14  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "15  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "16  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "17                                               None  \n",
       "18  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "19  D:\\Courses\\thesis\\data\\second\\models_turning_n...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PD_or_C</th>\n",
       "      <th>turning_angle</th>\n",
       "      <th>type_of_turn</th>\n",
       "      <th>n_files</th>\n",
       "      <th>model_saved</th>\n",
       "      <th>model_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>74</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>180_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>234</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>180_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C</td>\n",
       "      <td>225_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C</td>\n",
       "      <td>360_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C</td>\n",
       "      <td>90_degrees</td>\n",
       "      <td>-</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C</td>\n",
       "      <td>90_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>392</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C</td>\n",
       "      <td>90_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PD</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PD</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>97</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PD</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>33</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PD</td>\n",
       "      <td>180_degrees</td>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PD</td>\n",
       "      <td>180_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>209</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PD</td>\n",
       "      <td>180_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>76</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PD</td>\n",
       "      <td>225_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PD</td>\n",
       "      <td>270_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PD</td>\n",
       "      <td>90_degrees</td>\n",
       "      <td>-</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PD</td>\n",
       "      <td>90_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>404</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:48:21.087760Z",
     "start_time": "2025-08-14T19:48:21.072757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load a saved model (handles PyTorch 2.6+ weights_only change)\n",
    "import pickle\n",
    "\n",
    "def load_model_for_group(triple):\n",
    "    tag = \"__\".join(triple)\n",
    "    ckpt = MODEL_DIR / f\"TVAE_{tag}.pt\"\n",
    "    if not ckpt.exists():\n",
    "        raise FileNotFoundError(f\"No checkpoint for {triple}: {ckpt}\")\n",
    "\n",
    "    # Try normal load, then fallback to weights_only=False (safe if you trust your file)\n",
    "    try:\n",
    "        data = torch.load(ckpt, map_location=DEVICE)\n",
    "    except pickle.UnpicklingError:\n",
    "        data = torch.load(ckpt, map_location=DEVICE, weights_only=False)\n",
    "    except Exception as e:\n",
    "        # allowlist numpy reconstruct if needed\n",
    "        try:\n",
    "            from torch.serialization import add_safe_globals\n",
    "            import numpy as np\n",
    "            add_safe_globals([np.core.multiarray._reconstruct])\n",
    "            data = torch.load(ckpt, map_location=DEVICE)\n",
    "        except Exception:\n",
    "            raise e\n",
    "\n",
    "    model = TransformerVAE_BF(INPUT_DIM, win=data[\"meta\"][\"window\"]).to(DEVICE)\n",
    "    model.load_state_dict(data[\"model_state\"])\n",
    "    model.eval()\n",
    "    scaler = scaler_from_params(data[\"scaler_mean\"], data[\"scaler_scale\"])\n",
    "    header_cols = data.get(\"header_cols\", [f\"{ax}{i}\" for i in range(1, NUM_KPT+1) for ax in (\"x\",\"y\")])\n",
    "    return model, scaler, header_cols\n",
    "\n",
    "def generate_sequence(model, scaler, target_len, seed_seq=None, n_variants=1, temperature=0.95):\n",
    "    \"\"\"\n",
    "    Generate in NORMALIZED space; if 'seed_seq' provided, normalize by its frame-0\n",
    "    root & scale, then denormalize outputs back to those pixels so size stays constant.\n",
    "    \"\"\"\n",
    "    variants = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_variants):\n",
    "            if seed_seq is not None and len(seed_seq) >= WINDOW:\n",
    "                seed_norm, root0, scale0 = normalize_sequence_xy(seed_seq)          # (T,34), root, scale\n",
    "                seed_scaled = scaler.transform(seed_norm[:WINDOW])\n",
    "                seed_tensor = torch.tensor(seed_scaled[None], dtype=torch.float32, device=DEVICE)\n",
    "                mu, logvar = model.encode(seed_tensor)\n",
    "\n",
    "                current_window_norm = seed_norm[:WINDOW].copy()\n",
    "                out_frames = []\n",
    "                for _t in range(target_len):\n",
    "                    z = model.reparameterize(mu, logvar, temperature=temperature)\n",
    "                    synth_scaled = model.decode(z)[0].detach().cpu().numpy()             # (T,F) standardized\n",
    "                    frame_norm = scaler.inverse_transform(synth_scaled[-1][None])[0]     # (34,) normalized\n",
    "                    frame_pix  = denormalize_frame_34(frame_norm, root0, scale0)        # keep seed size\n",
    "                    out_frames.append(frame_pix)\n",
    "\n",
    "                    current_window_norm = np.vstack([current_window_norm, frame_norm])[-WINDOW:]\n",
    "                    win_scaled = scaler.transform(current_window_norm)\n",
    "                    mu, logvar = model.encode(torch.tensor(win_scaled[None], dtype=torch.float32, device=DEVICE))\n",
    "                variants.append(np.array(out_frames))\n",
    "\n",
    "            else:\n",
    "                # Seedless: unit scale at origin (you can swap in a real seed's root/scale if desired)\n",
    "                latent_dim = model.fc_latent.in_features\n",
    "                z0 = torch.randn(1, latent_dim, device=DEVICE)\n",
    "                synth_scaled = model.decode(z0)[0].detach().cpu().numpy()\n",
    "                current_window_norm = scaler.inverse_transform(synth_scaled)   # (T,F)\n",
    "\n",
    "                root0 = np.zeros(2, dtype=np.float32); scale0 = 1.0\n",
    "                out_frames = []\n",
    "                win_scaled = scaler.transform(current_window_norm[-WINDOW:])\n",
    "                mu, logvar = model.encode(torch.tensor(win_scaled[None], dtype=torch.float32, device=DEVICE))\n",
    "\n",
    "                for _t in range(target_len):\n",
    "                    z = model.reparameterize(mu, logvar, temperature=temperature)\n",
    "                    synth_scaled = model.decode(z)[0].detach().cpu().numpy()\n",
    "                    frame_norm = scaler.inverse_transform(synth_scaled[-1][None])[0]\n",
    "                    frame_pix  = denormalize_frame_34(frame_norm, root0, scale0)\n",
    "                    out_frames.append(frame_pix)\n",
    "\n",
    "                    current_window_norm = np.vstack([current_window_norm, frame_norm])[-WINDOW:]\n",
    "                    win_scaled = scaler.transform(current_window_norm)\n",
    "                    mu, logvar = model.encode(torch.tensor(win_scaled[None], dtype=torch.float32, device=DEVICE))\n",
    "\n",
    "                variants.append(np.array(out_frames))\n",
    "    return variants\n"
   ],
   "id": "88e22718c54dab97",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:48:25.284990Z",
     "start_time": "2025-08-14T19:48:24.833969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage: set to any trained triple from the manifest\n",
    "CHOSEN_GROUP = ('PD', '180_degrees', 'pivot_turn')   # <-- edit to a trained triple\n",
    "\n",
    "try:\n",
    "    model, scaler, header_cols = load_model_for_group(CHOSEN_GROUP)\n",
    "    group_path = GROUPED_DIR / CHOSEN_GROUP[0] / CHOSEN_GROUP[1] / CHOSEN_GROUP[2]\n",
    "    some_file = next(group_path.glob(\"*.csv\"))\n",
    "    seed_arr, _ = read_keypoints_csv(some_file, INPUT_DIM)\n",
    "\n",
    "    target_len = len(seed_arr)\n",
    "\n",
    "    # Seeded (keeps person size constant from the seed)\n",
    "    variants = generate_sequence(model, scaler, target_len=target_len, seed_seq=seed_arr,\n",
    "                                 n_variants=1, temperature=0.95)\n",
    "    synth = variants[0]\n",
    "    out_csv = SYN_DIR / f\"synthetic_{CHOSEN_GROUP[0]}__{CHOSEN_GROUP[1]}__{CHOSEN_GROUP[2]}_seeded_norm.csv\"\n",
    "    pd.DataFrame(synth, columns=header_cols).to_csv(out_csv, index=False, float_format=\"%.1f\")\n",
    "    print(\"Saved (seeded):\", out_csv)\n",
    "\n",
    "    # Seedless (unit-size stick figure at origin)\n",
    "    variants2 = generate_sequence(model, scaler, target_len=target_len, seed_seq=None,\n",
    "                                  n_variants=1, temperature=0.95)\n",
    "    synth2 = variants2[0]\n",
    "    out_csv2 = SYN_DIR / f\"synthetic_{CHOSEN_GROUP[0]}__{CHOSEN_GROUP[1]}__{CHOSEN_GROUP[2]}_seedless_norm.csv\"\n",
    "    pd.DataFrame(synth2, columns=header_cols).to_csv(out_csv2, index=False, float_format=\"%.1f\")\n",
    "    print(\"Saved (seedless):\", out_csv2)\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"No CSV found in the chosen group's folder. Pick another CHOSEN_GROUP.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n"
   ],
   "id": "c05608db8b25694a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved (seeded): D:\\Courses\\thesis\\data\\second\\synthetic_turning_norm_250\\synthetic_PD__180_degrees__pivot_turn_seeded_norm.csv\n",
      "Saved (seedless): D:\\Courses\\thesis\\data\\second\\synthetic_turning_norm_250\\synthetic_PD__180_degrees__pivot_turn_seedless_norm.csv\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T19:48:40.296833Z",
     "start_time": "2025-08-14T19:48:38.210196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate N seeded + seedless variants from one chosen group\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "CHOSEN_GROUP = ('PD', '180_degrees', 'pivot_turn')  # edit\n",
    "N_SEEDED_VARIANTS   = 10\n",
    "N_SEEDLESS_VARIANTS = 5\n",
    "TEMP = 0.9\n",
    "#LOCK_MODE = None   # \"per_frame_root\", \"anchor\", or None\n",
    "FIXED_SEED = 123               # for reproducibility; set to None to randomize\n",
    "\n",
    "if FIXED_SEED is not None:\n",
    "    random.seed(FIXED_SEED); np.random.seed(FIXED_SEED); torch.manual_seed(FIXED_SEED)\n",
    "\n",
    "# load model + a seed file\n",
    "model, scaler, header_cols = load_model_for_group(CHOSEN_GROUP)\n",
    "group_path = GROUPED_DIR / CHOSEN_GROUP[0] / CHOSEN_GROUP[1] / CHOSEN_GROUP[2]\n",
    "seed_file = sorted(group_path.glob(\"*.csv\"))[0]  # or pick a specific one\n",
    "seed_arr, _ = read_keypoints_csv(seed_file, INPUT_DIM)\n",
    "target_len = len(seed_arr)\n",
    "\n",
    "print(\"Seed file:\", seed_file.name, \"frames:\", target_len)\n",
    "\n",
    "# ---- seeded variants ----\n",
    "for v in range(N_SEEDED_VARIANTS):\n",
    "    [synth] = generate_sequence(\n",
    "        model, scaler, target_len=target_len,\n",
    "        seed_seq=seed_arr, n_variants=1,\n",
    "        temperature=TEMP\n",
    "    )\n",
    "    out_csv = SYN_DIR / f\"synthetic_{CHOSEN_GROUP[0]}__{CHOSEN_GROUP[1]}__{CHOSEN_GROUP[2]}__seeded_v{v:02d}.csv\"\n",
    "    pd.DataFrame(synth, columns=header_cols).to_csv(out_csv, index=False, float_format=\"%.1f\")\n",
    "    print(\"Saved:\", out_csv.name)\n",
    "\n",
    "# ---- seedless variants ----\n",
    "for v in range(N_SEEDLESS_VARIANTS):\n",
    "    [synth] = generate_sequence(\n",
    "        model, scaler, target_len=target_len,\n",
    "        seed_seq=None, n_variants=1,\n",
    "        temperature=TEMP\n",
    "    )\n",
    "    out_csv = SYN_DIR / f\"synthetic_{CHOSEN_GROUP[0]}__{CHOSEN_GROUP[1]}__{CHOSEN_GROUP[2]}__seedless_v{v:02d}.csv\"\n",
    "    pd.DataFrame(synth, columns=header_cols).to_csv(out_csv, index=False, float_format=\"%.1f\")\n",
    "    print(\"Saved:\", out_csv.name)\n"
   ],
   "id": "d2be49c381db1ec9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed file: Pt253_PD_n_3482.csv frames: 72\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seeded_v00.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seeded_v01.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seeded_v02.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seeded_v03.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seeded_v04.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seeded_v05.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seeded_v06.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seeded_v07.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seeded_v08.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seeded_v09.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seedless_v00.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seedless_v01.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seedless_v02.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seedless_v03.csv\n",
      "Saved: synthetic_PD__180_degrees__pivot_turn__seedless_v04.csv\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
