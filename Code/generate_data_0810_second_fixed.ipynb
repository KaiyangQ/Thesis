{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-14T16:59:26.573562Z",
     "start_time": "2025-08-14T16:59:22.633545Z"
    }
   },
   "source": [
    "# Imports & Config\n",
    "import os, re, math, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths (edit as needed)\n",
    "GROUPED_DIR = Path(r\"D:\\Courses\\thesis\\data\\turning_keypoints_grouped\")   # grouped by (PD_or_C / angle / type_of_turn)\n",
    "MODEL_DIR   = Path(r\"D:\\Courses\\thesis\\data\\second\\models_turning_norm_fixed\")         # where to save models (normalized version)\n",
    "SYN_DIR     = Path(r\"D:\\Courses\\thesis\\data\\second\\synthetic_turning_norm_fixed\")      # where to save generated CSVs\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SYN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data/Model params\n",
    "NUM_KPT   = 17\n",
    "INPUT_DIM = NUM_KPT * 2\n",
    "WINDOW    = 10\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS     = 25\n",
    "LR         = 1e-3\n",
    "SEED       = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "print(\"Device:\", DEVICE)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T16:59:26.604752Z",
     "start_time": "2025-08-14T16:59:26.592080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Skeleton graph + root joint used for normalization\n",
    "connections = [\n",
    "    [0, 1], [1, 2], [2, 3],\n",
    "    [0, 4], [4, 5], [5, 6],\n",
    "    [0, 7], [7, 8], [8, 9], [9, 10],\n",
    "    [8, 11], [11, 12], [12, 13],\n",
    "    [8, 14], [14, 15], [15, 16],\n",
    "]\n",
    "ROOT = 8  # central joint (hub) in this graph\n"
   ],
   "id": "ebb1a8ef7927115b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T16:59:26.839055Z",
     "start_time": "2025-08-14T16:59:26.824628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List groups and parse subject id from filename\n",
    "def list_groups(grouped_dir: Path):\n",
    "    \"\"\"Return list of (pd_or_c, angle, turn_type, path, n_files).\"\"\"\n",
    "    triples = []\n",
    "    for pdc_dir in sorted([p for p in grouped_dir.iterdir() if p.is_dir()]):\n",
    "        for angle_dir in sorted([p for p in pdc_dir.iterdir() if p.is_dir()]):\n",
    "            for ttype_dir in sorted([p for p in angle_dir.iterdir() if p.is_dir()]):\n",
    "                n = len(list(ttype_dir.glob(\"*.csv\")))\n",
    "                if n > 0:\n",
    "                    triples.append((pdc_dir.name, angle_dir.name, ttype_dir.name, ttype_dir, n))\n",
    "    return triples\n",
    "\n",
    "def parse_subject_id_from_filename(name: str):\n",
    "    # e.g., Pt204_C_n_350.csv -> 204\n",
    "    m = re.match(r\"Pt(\\d+)_\", name)\n",
    "    if m:\n",
    "        try: return int(m.group(1))\n",
    "        except: return None\n",
    "    return None\n"
   ],
   "id": "9f3485a7f7b1fb4c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T16:59:26.931174Z",
     "start_time": "2025-08-14T16:59:26.918667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PATCH: headerless-safe CSV loader + group loader (overrides previous versions)\n",
    "\n",
    "def _has_numeric_header(df) -> bool:\n",
    "    \"\"\"True if most column labels look like numbers (means we accidentally used row 1 as header).\"\"\"\n",
    "    if len(df.columns) == 0:\n",
    "        return False\n",
    "    num_like = 0\n",
    "    for c in df.columns:\n",
    "        try:\n",
    "            float(str(c))\n",
    "            num_like += 1\n",
    "        except:\n",
    "            pass\n",
    "    return (num_like / len(df.columns)) >= 0.8\n",
    "\n",
    "def read_keypoints_csv(path: Path, input_dim=34):\n",
    "    \"\"\"\n",
    "    Robust loader for headerless files:\n",
    "    - If the first read looks like it has numeric headers, re-read with header=None.\n",
    "    - Always select the first `input_dim` numeric columns by position.\n",
    "    - Drop NaN rows.\n",
    "    - Require at least WINDOW+2 frames.\n",
    "    - Return synthetic column names x1,y1,...,x17,y17 for consistency.\n",
    "    \"\"\"\n",
    "    # first try (might wrongly treat first row as header)\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    if _has_numeric_header(df):\n",
    "        # re-read with no header so the first row is data\n",
    "        df = pd.read_csv(path, header=None, low_memory=False)\n",
    "\n",
    "    num_df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    if num_df.shape[1] < input_dim:\n",
    "        return None\n",
    "\n",
    "    use = num_df.iloc[:, :input_dim].dropna()\n",
    "    arr = use.to_numpy(dtype=np.float32)\n",
    "    if arr.shape[0] < WINDOW + 2:\n",
    "        return None\n",
    "\n",
    "    # synthetic XY names for saving later\n",
    "    cols = []\n",
    "    for k in range(1, NUM_KPT + 1):\n",
    "        cols += [f\"x{k}\", f\"y{k}\"]\n",
    "\n",
    "    return arr, cols\n",
    "\n",
    "def load_group_sequences(group_path: Path, input_dim=34):\n",
    "    \"\"\"\n",
    "    Load all CSVs in the group using the headerless-safe reader.\n",
    "    Do NOT enforce a shared header schema; we always output the same synthetic XY names.\n",
    "    \"\"\"\n",
    "    file_list, seqs, headers, subjs = [], [], [], []\n",
    "    for f in sorted(group_path.glob(\"*.csv\")):\n",
    "        out = read_keypoints_csv(f, input_dim=input_dim)\n",
    "        if out is None:\n",
    "            continue\n",
    "        arr, cols = out\n",
    "        file_list.append(f.name)\n",
    "        seqs.append(arr)\n",
    "        headers.append(cols)  # all identical synthetic headers\n",
    "        subjs.append(parse_subject_id_from_filename(f.name))\n",
    "    return file_list, seqs, headers, subjs\n"
   ],
   "id": "e147ff5f449dedc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T16:59:26.962422Z",
     "start_time": "2025-08-14T16:59:26.947412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalization utilities (sequence-level) + bone-length helpers\n",
    "def _median_bone_scale_2d(points_17x2):\n",
    "    centered = points_17x2 - points_17x2[ROOT]\n",
    "    dists = [np.linalg.norm(centered[j] - centered[i]) for i, j in connections]\n",
    "    return np.median(dists) if len(dists) else 1.0\n",
    "\n",
    "def normalize_sequence_xy(seq_Tx34):\n",
    "    \"\"\"\n",
    "    Per-sequence normalization using frame 0:\n",
    "    - subtract root joint of frame 0 (translation)\n",
    "    - divide by median bone length of frame 0 (scale)\n",
    "    Returns: seq_norm (T,34), root0 (2,), scale0 (float)\n",
    "    \"\"\"\n",
    "    T = seq_Tx34.shape[0]\n",
    "    pts0 = seq_Tx34[0].reshape(17, 2)\n",
    "    root0 = pts0[ROOT].copy()\n",
    "    s0 = _median_bone_scale_2d(pts0)\n",
    "    if not np.isfinite(s0) or s0 <= 1e-6:\n",
    "        s0 = 1.0\n",
    "\n",
    "    seq = seq_Tx34.reshape(T, 17, 2)\n",
    "    seq_centered = seq - root0\n",
    "    seq_scaled = seq_centered / s0\n",
    "    return seq_scaled.reshape(T, 34), root0, float(s0)\n",
    "\n",
    "def denormalize_frame_34(frame_34, root0, scale0):\n",
    "    pts = frame_34.reshape(17, 2) * scale0 + root0\n",
    "    return pts.reshape(34,)\n",
    "\n",
    "def bone_lengths_batch(x_B_T_34):\n",
    "    \"\"\"x: (B,T,34) in normalized coords -> (B,T, num_bones) lengths.\"\"\"\n",
    "    B, T, F = x_B_T_34.shape\n",
    "    coords = x_B_T_34.view(B, T, 17, 2)\n",
    "    lens = []\n",
    "    for (i, j) in connections:\n",
    "        diff = coords[:, :, j, :] - coords[:, :, i, :]\n",
    "        l = torch.norm(diff, dim=-1)  # (B,T)\n",
    "        lens.append(l.unsqueeze(-1))\n",
    "    return torch.cat(lens, dim=-1)\n"
   ],
   "id": "ce4b61e62158f122",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T16:59:26.994032Z",
     "start_time": "2025-08-14T16:59:26.979434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split (subject-aware), window count, scaler utils\n",
    "def split_subject_aware(file_list, seqs, subjs, test_size=0.15, seed=42):\n",
    "    man = pd.DataFrame({\"idx\": range(len(file_list)), \"subj\": subjs})\n",
    "    known = man[man[\"subj\"].notna()]\n",
    "    unknown = man[man[\"subj\"].isna()]\n",
    "\n",
    "    train_idx, val_idx = [], []\n",
    "\n",
    "    if len(known[\"subj\"].unique()) >= 2:\n",
    "        tr_subj, va_subj = train_test_split(sorted(known[\"subj\"].unique()),\n",
    "                                            test_size=test_size, random_state=seed, shuffle=True)\n",
    "        train_idx += known[known[\"subj\"].isin(tr_subj)][\"idx\"].tolist()\n",
    "        val_idx   += known[known[\"subj\"].isin(va_subj)][\"idx\"].tolist()\n",
    "    else:\n",
    "        if len(known) >= 2:\n",
    "            tr, va = train_test_split(known[\"idx\"].tolist(),\n",
    "                                      test_size=max(1/len(known), test_size),\n",
    "                                      random_state=seed, shuffle=True)\n",
    "            train_idx += tr; val_idx += va\n",
    "\n",
    "    if len(unknown) >= 2:\n",
    "        tr, va = train_test_split(unknown[\"idx\"].tolist(),\n",
    "                                  test_size=max(1/len(unknown), test_size),\n",
    "                                  random_state=seed, shuffle=True)\n",
    "        train_idx += tr; val_idx += va\n",
    "    else:\n",
    "        train_idx += unknown[\"idx\"].tolist()\n",
    "\n",
    "    if len(val_idx) == 0:\n",
    "        train_idx = man[\"idx\"].tolist()\n",
    "        val_idx = []\n",
    "\n",
    "    train_seqs = [seqs[i] for i in train_idx]\n",
    "    val_seqs   = [seqs[i] for i in val_idx]\n",
    "    return train_seqs, val_seqs, train_idx, val_idx\n",
    "\n",
    "def make_windows_count(seqs, win):\n",
    "    return sum(max(0, len(s)-win) for s in seqs)\n",
    "\n",
    "def scaler_from_params(mean, scale):\n",
    "    sc = StandardScaler()\n",
    "    sc.mean_ = np.array(mean, dtype=np.float64)\n",
    "    sc.scale_ = np.array(scale, dtype=np.float64)\n",
    "    sc.var_ = sc.scale_**2\n",
    "    sc.n_features_in_ = len(sc.mean_)\n",
    "    return sc\n"
   ],
   "id": "74b9eeb46104e076",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T16:59:27.025551Z",
     "start_time": "2025-08-14T16:59:27.010542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transformer-VAE (batch_first=True)\n",
    "class PositionalEncodingBF(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1,L,D)\n",
    "    def forward(self, x):  # x: (B,T,D)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerVAE_BF(nn.Module):\n",
    "    def __init__(self, input_dim, win=WINDOW, d_model=96, nhead=6, num_layers=3, latent_dim=48):\n",
    "        super().__init__()\n",
    "        self.win = win\n",
    "        self.input_linear = nn.Linear(input_dim, d_model)\n",
    "        self.pos_enc = PositionalEncodingBF(d_model, max_len=win)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.fc_mu = nn.Linear(d_model * win, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(d_model * win, latent_dim)\n",
    "        self.fc_latent = nn.Linear(latent_dim, d_model * win)\n",
    "        dec_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=num_layers)\n",
    "        self.output_linear = nn.Linear(d_model, input_dim)\n",
    "    def encode(self, x):\n",
    "        h = self.input_linear(x)\n",
    "        h = self.pos_enc(h)\n",
    "        out = self.encoder(h)                # (B,T,D)\n",
    "        flat = out.reshape(out.size(0), -1)\n",
    "        return self.fc_mu(flat), self.fc_logvar(flat)\n",
    "    def reparameterize(self, mu, logvar, temperature=1.0):\n",
    "        std = torch.exp(0.5 * logvar) * temperature\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def decode(self, z):\n",
    "        x = self.fc_latent(z).view(z.size(0), self.win, -1)\n",
    "        tgt = self.pos_enc(x)\n",
    "        memory = torch.zeros(z.size(0), self.win, tgt.size(2), device=z.device)  # (B,T,D)\n",
    "        out = self.decoder(tgt, memory)\n",
    "        return self.output_linear(out)\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n"
   ],
   "id": "acdde5f7934968bb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T16:59:27.057069Z",
     "start_time": "2025-08-14T16:59:27.042055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss: reconstruction + KL + velocity + bone-length consistency\n",
    "def loss_with_bone(recon_x, x, mu, logvar, w_kld=0.1, w_vel=0.15, w_bone=0.15):\n",
    "    \"\"\"\n",
    "    x, recon_x: (B,T,34) in normalized coords\n",
    "    \"\"\"\n",
    "    mse = F.mse_loss(recon_x, x)\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    vel_orig = torch.diff(x, dim=1)\n",
    "    vel_recon = torch.diff(recon_x, dim=1)\n",
    "    vel = F.mse_loss(vel_recon, vel_orig)\n",
    "\n",
    "    bl_orig = bone_lengths_batch(x)\n",
    "    bl_recon = bone_lengths_batch(recon_x)\n",
    "    bone = F.mse_loss(bl_recon, bl_orig)\n",
    "\n",
    "    return mse + w_kld * kld + w_vel * vel + w_bone * bone\n"
   ],
   "id": "3666721ce8e0c6ca",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T16:59:27.088585Z",
     "start_time": "2025-08-14T16:59:27.074079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train one group (normalized pipeline) and save checkpoint\n",
    "class _WindowDataset(Dataset):\n",
    "    def __init__(self, seqs_norm, scaler, win):\n",
    "        xs = []\n",
    "        for s in seqs_norm:\n",
    "            s2 = scaler.transform(s)  # standardize after our normalization\n",
    "            for i in range(win, len(s2)):\n",
    "                xs.append(s2[i - win:i])\n",
    "        self.samples = np.asarray(xs, np.float32)\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, i): return torch.tensor(self.samples[i], dtype=torch.float32)\n",
    "\n",
    "def train_one_group(triple, group_path: Path):\n",
    "    file_list, seqs_raw, headers, subjs = load_group_sequences(group_path, INPUT_DIM)\n",
    "    if len(seqs_raw) == 0:\n",
    "        print(f\"  [SKIP empty] {triple}\")\n",
    "        return None\n",
    "\n",
    "    # 1) Per-sequence normalization\n",
    "    seqs_norm = []\n",
    "    for arr in seqs_raw:\n",
    "        s_norm, _, _ = normalize_sequence_xy(arr)  # ignore root/scale during training\n",
    "        seqs_norm.append(s_norm)\n",
    "\n",
    "    # 2) Subject-aware split\n",
    "    _, _, train_idx, val_idx = split_subject_aware(file_list, seqs_norm, subjs, test_size=0.15, seed=SEED)\n",
    "    train_seqs = [seqs_norm[i] for i in train_idx] if len(train_idx) else seqs_norm\n",
    "    val_seqs   = [seqs_norm[i] for i in val_idx]   if len(val_idx)   else []\n",
    "\n",
    "    # enough windows?\n",
    "    if make_windows_count(train_seqs, WINDOW) < 64:\n",
    "        print(f\"  [SKIP small] {triple} — not enough windows\")\n",
    "        return None\n",
    "\n",
    "    # 3) Fit scaler on normalized train data\n",
    "    scaler = StandardScaler().fit(np.concatenate(train_seqs, axis=0))\n",
    "\n",
    "    # 4) Dataloaders\n",
    "    train_loader = DataLoader(_WindowDataset(train_seqs, scaler, WINDOW),\n",
    "                              batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(_WindowDataset(val_seqs, scaler, WINDOW),\n",
    "                            batch_size=BATCH_SIZE, shuffle=False, drop_last=False) if len(val_seqs)>0 else None\n",
    "\n",
    "    # 5) Model\n",
    "    model = TransformerVAE_BF(INPUT_DIM, win=WINDOW).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    best_val = float('inf'); best_state = None\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train(); total = 0.0\n",
    "        for b in train_loader:\n",
    "            b = b.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            recon, mu, logvar = model(b)\n",
    "            loss = loss_with_bone(recon, b, mu, logvar)\n",
    "            loss.backward(); opt.step()\n",
    "            total += loss.item()\n",
    "\n",
    "        if (epoch % 5 == 0) or epoch == 1:\n",
    "            if val_loader and len(val_loader) > 0:\n",
    "                model.eval(); vtot = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for vb in val_loader:\n",
    "                        vb = vb.to(DEVICE)\n",
    "                        r, m, l = model(vb)\n",
    "                        vtot += loss_with_bone(r, vb, m, l).item()\n",
    "                vloss = vtot / len(val_loader)\n",
    "            else:\n",
    "                vloss = float('nan')\n",
    "\n",
    "            print(f\"  Epoch {epoch:02d}/{EPOCHS} — train {total/len(train_loader):.5f} val {vloss:.5f}\")\n",
    "            if not math.isnan(vloss) and vloss < best_val:\n",
    "                best_val = vloss; best_state = model.state_dict()\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # save checkpoint\n",
    "    header_series = pd.Series([\"|\".join(h) for h in headers if h])\n",
    "    header_cols = header_series.mode().iloc[0].split(\"|\") if not header_series.empty \\\n",
    "                  else [f\"{ax}{i}\" for i in range(1, NUM_KPT+1) for ax in (\"x\",\"y\")]\n",
    "    tag = \"__\".join(triple)\n",
    "    ckpt = MODEL_DIR / f\"TVAE_{tag}.pt\"\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"scaler_mean\": scaler.mean_,\n",
    "        \"scaler_scale\": scaler.scale_,\n",
    "        \"header_cols\": header_cols,\n",
    "        \"meta\": {\n",
    "            \"group\": triple,\n",
    "            \"input_dim\": INPUT_DIM,\n",
    "            \"window\": WINDOW,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"best_val\": None if math.isinf(best_val) else float(best_val),\n",
    "            \"date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"normalized\": True,\n",
    "            \"root_index\": ROOT,\n",
    "            \"connections\": connections,\n",
    "        }\n",
    "    }, ckpt)\n",
    "    return ckpt\n"
   ],
   "id": "701b5a124b604db9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:07:25.169121Z",
     "start_time": "2025-08-14T16:59:27.105132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train all groups once & write a manifest\n",
    "groups = list_groups(GROUPED_DIR)\n",
    "print(f\"Discovered {len(groups)} groups.\")\n",
    "\n",
    "manifest_rows = []\n",
    "for pd_or_c, angle, ttype, path, n_files in groups:\n",
    "    triple = (pd_or_c, angle, ttype)\n",
    "    tag = \"__\".join(triple)\n",
    "    ckpt_path = MODEL_DIR / f\"TVAE_{tag}.pt\"\n",
    "    if ckpt_path.exists():\n",
    "        print(f\"[SKIP existing] {triple}\")\n",
    "        saved = True\n",
    "        ckpt = ckpt_path\n",
    "    else:\n",
    "        print(f\"[TRAIN] {triple}  ({n_files} files)\")\n",
    "        try:\n",
    "            ckpt = train_one_group(triple, path)\n",
    "            saved = ckpt is not None\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR training {triple}: {e}\")\n",
    "            ckpt = None\n",
    "            saved = False\n",
    "\n",
    "    manifest_rows.append({\n",
    "        \"PD_or_C\": pd_or_c,\n",
    "        \"turning_angle\": angle,\n",
    "        \"type_of_turn\": ttype,\n",
    "        \"n_files\": n_files,\n",
    "        \"model_saved\": saved,\n",
    "        \"model_path\": str(ckpt) if ckpt else None,\n",
    "    })\n",
    "\n",
    "manifest = pd.DataFrame(manifest_rows)\n",
    "manifest_csv = MODEL_DIR / \"_models_manifest.csv\"\n",
    "manifest.to_csv(manifest_csv, index=False)\n",
    "print(\"\\nSaved model manifest:\", manifest_csv)\n",
    "display(manifest.head(20))\n"
   ],
   "id": "f19e982534d98c73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 21 groups.\n",
      "[TRAIN] ('C', '135_degrees', '-')  (1 files)\n",
      "  [SKIP small] ('C', '135_degrees', '-') — not enough windows\n",
      "[TRAIN] ('C', '135_degrees', 'pivot_turn')  (74 files)\n",
      "  Epoch 01/25 — train 0.90586 val 1.39194\n",
      "  Epoch 05/25 — train 0.40826 val 0.98006\n",
      "  Epoch 10/25 — train 0.38053 val 0.95755\n",
      "  Epoch 15/25 — train 0.37228 val 0.89516\n",
      "  Epoch 20/25 — train 0.35716 val 0.83862\n",
      "  Epoch 25/25 — train 0.33993 val 0.88387\n",
      "[TRAIN] ('C', '135_degrees', 'step_turn')  (3 files)\n",
      "  Epoch 01/25 — train 1.41913 val 2.93284\n",
      "  Epoch 05/25 — train 0.97472 val 2.16765\n",
      "  Epoch 10/25 — train 0.57049 val 2.28231\n",
      "  Epoch 15/25 — train 0.57934 val 3.12435\n",
      "  Epoch 20/25 — train 0.59159 val 2.68797\n",
      "  Epoch 25/25 — train 0.59411 val 2.65896\n",
      "[TRAIN] ('C', '180_degrees', 'pivot_turn')  (234 files)\n",
      "  Epoch 01/25 — train 0.52667 val 0.54862\n",
      "  Epoch 05/25 — train 0.19266 val 0.27212\n",
      "  Epoch 10/25 — train 0.15551 val 0.23991\n",
      "  Epoch 15/25 — train 0.12191 val 0.19740\n",
      "  Epoch 20/25 — train 0.10182 val 0.16656\n",
      "  Epoch 25/25 — train 0.09453 val 0.16452\n",
      "[TRAIN] ('C', '180_degrees', 'step_turn')  (12 files)\n",
      "  Epoch 01/25 — train 1.38703 val 40.83769\n",
      "  Epoch 05/25 — train 0.99154 val 40.06035\n",
      "  Epoch 10/25 — train 0.89996 val 39.86506\n",
      "  Epoch 15/25 — train 0.54213 val 37.50708\n",
      "  Epoch 20/25 — train 0.70513 val 36.94126\n",
      "  Epoch 25/25 — train 0.66509 val 36.72425\n",
      "[TRAIN] ('C', '225_degrees', 'pivot_turn')  (3 files)\n",
      "  Epoch 01/25 — train 1.42482 val 1.54071\n",
      "  Epoch 05/25 — train 0.82156 val 1.45338\n",
      "  Epoch 10/25 — train 0.55364 val 1.80950\n",
      "  Epoch 15/25 — train 0.45297 val 1.54910\n",
      "  Epoch 20/25 — train 0.34350 val 1.51631\n",
      "  Epoch 25/25 — train 0.29531 val 1.42234\n",
      "[TRAIN] ('C', '360_degrees', 'pivot_turn')  (1 files)\n",
      "  Epoch 01/25 — train 1.42619 val nan\n",
      "  Epoch 05/25 — train 1.13434 val nan\n",
      "  Epoch 10/25 — train 1.00146 val nan\n",
      "  Epoch 15/25 — train 0.81366 val nan\n",
      "  Epoch 20/25 — train 0.83942 val nan\n",
      "  Epoch 25/25 — train 0.86318 val nan\n",
      "[TRAIN] ('C', '90_degrees', '-')  (10 files)\n",
      "  Epoch 01/25 — train 1.21065 val 1.62060\n",
      "  Epoch 05/25 — train 0.79108 val 0.78231\n",
      "  Epoch 10/25 — train 0.54581 val 0.58065\n",
      "  Epoch 15/25 — train 0.45006 val 0.53329\n",
      "  Epoch 20/25 — train 0.41150 val 0.91703\n",
      "  Epoch 25/25 — train 0.50598 val 0.66809\n",
      "[TRAIN] ('C', '90_degrees', 'pivot_turn')  (392 files)\n",
      "  Epoch 01/25 — train 0.74717 val 0.75339\n",
      "  Epoch 05/25 — train 0.21735 val 0.24427\n",
      "  Epoch 10/25 — train 0.14782 val 0.17022\n",
      "  Epoch 15/25 — train 0.12975 val 0.18753\n",
      "  Epoch 20/25 — train 0.11905 val 0.16400\n",
      "  Epoch 25/25 — train 0.11698 val 0.16238\n",
      "[TRAIN] ('C', '90_degrees', 'step_turn')  (13 files)\n",
      "  Epoch 01/25 — train 1.13274 val 0.53787\n",
      "  Epoch 05/25 — train 0.82187 val 0.60705\n",
      "  Epoch 10/25 — train 0.55561 val 0.46880\n",
      "  Epoch 15/25 — train 0.42065 val 0.51300\n",
      "  Epoch 20/25 — train 0.37358 val 0.42229\n",
      "  Epoch 25/25 — train 0.40484 val 0.38866\n",
      "[TRAIN] ('PD', '135_degrees', '-')  (4 files)\n",
      "  Epoch 01/25 — train 1.39301 val 21.17306\n",
      "  Epoch 05/25 — train 1.05870 val 21.57393\n",
      "  Epoch 10/25 — train 1.00969 val 21.28469\n",
      "  Epoch 15/25 — train 0.70739 val 19.03662\n",
      "  Epoch 20/25 — train 0.63569 val 18.10999\n",
      "  Epoch 25/25 — train 0.64295 val 19.38407\n",
      "[TRAIN] ('PD', '135_degrees', 'pivot_turn')  (97 files)\n",
      "  Epoch 01/25 — train 0.55622 val 0.28741\n",
      "  Epoch 05/25 — train 0.21252 val 0.24435\n",
      "  Epoch 10/25 — train 0.18708 val 0.21960\n",
      "  Epoch 15/25 — train 0.12959 val 0.16338\n",
      "  Epoch 20/25 — train 0.10339 val 0.15700\n",
      "  Epoch 25/25 — train 0.09864 val 0.15863\n",
      "[TRAIN] ('PD', '135_degrees', 'step_turn')  (33 files)\n",
      "  Epoch 01/25 — train 0.92725 val 0.53870\n",
      "  Epoch 05/25 — train 0.28843 val 0.25898\n",
      "  Epoch 10/25 — train 0.20909 val 0.24202\n",
      "  Epoch 15/25 — train 0.19194 val 0.23524\n",
      "  Epoch 20/25 — train 0.16838 val 0.23220\n",
      "  Epoch 25/25 — train 0.12042 val 0.20806\n",
      "[TRAIN] ('PD', '180_degrees', '-')  (4 files)\n",
      "  Epoch 01/25 — train 1.31010 val 1.53255\n",
      "  Epoch 05/25 — train 0.93000 val 1.56924\n",
      "  Epoch 10/25 — train 0.72425 val 1.22902\n",
      "  Epoch 15/25 — train 0.50120 val 1.13219\n",
      "  Epoch 20/25 — train 0.44174 val 1.13708\n",
      "  Epoch 25/25 — train 0.40778 val 1.04001\n",
      "[TRAIN] ('PD', '180_degrees', 'pivot_turn')  (209 files)\n",
      "  Epoch 01/25 — train 0.75574 val 0.35635\n",
      "  Epoch 05/25 — train 0.28356 val 0.22814\n",
      "  Epoch 10/25 — train 0.22152 val 0.19390\n",
      "  Epoch 15/25 — train 0.20052 val 0.18227\n",
      "  Epoch 20/25 — train 0.18563 val 0.18035\n",
      "  Epoch 25/25 — train 0.18801 val 0.19212\n",
      "[TRAIN] ('PD', '180_degrees', 'step_turn')  (76 files)\n",
      "  Epoch 01/25 — train 0.90562 val 1.49276\n",
      "  Epoch 05/25 — train 0.79902 val 1.51079\n",
      "  Epoch 10/25 — train 0.82066 val 1.32217\n",
      "  Epoch 15/25 — train 0.79666 val 1.39682\n",
      "  Epoch 20/25 — train 0.84783 val 1.31920\n",
      "  Epoch 25/25 — train 0.88253 val 1.65543\n",
      "[TRAIN] ('PD', '225_degrees', 'step_turn')  (2 files)\n",
      "  Epoch 01/25 — train 1.52911 val 72.99599\n",
      "  Epoch 05/25 — train 0.99423 val 73.38351\n",
      "  Epoch 10/25 — train 0.95651 val 73.01777\n",
      "  Epoch 15/25 — train 0.81070 val 70.59493\n",
      "  Epoch 20/25 — train 0.62602 val 68.48066\n",
      "  Epoch 25/25 — train 0.61321 val 68.56455\n",
      "[TRAIN] ('PD', '270_degrees', 'pivot_turn')  (1 files)\n",
      "  [SKIP small] ('PD', '270_degrees', 'pivot_turn') — not enough windows\n",
      "[TRAIN] ('PD', '90_degrees', '-')  (18 files)\n",
      "  Epoch 01/25 — train 0.92925 val 0.69069\n",
      "  Epoch 05/25 — train 0.47002 val 0.45053\n",
      "  Epoch 10/25 — train 0.36089 val 0.35610\n",
      "  Epoch 15/25 — train 0.27243 val 0.40883\n",
      "  Epoch 20/25 — train 0.26002 val 0.59831\n",
      "  Epoch 25/25 — train 0.21372 val 0.45251\n",
      "[TRAIN] ('PD', '90_degrees', 'pivot_turn')  (404 files)\n",
      "  Epoch 01/25 — train 0.42515 val 0.35833\n",
      "  Epoch 05/25 — train 0.21905 val 0.24718\n",
      "  Epoch 10/25 — train 0.12817 val 0.14766\n",
      "  Epoch 15/25 — train 0.10016 val 0.13891\n",
      "  Epoch 20/25 — train 0.09281 val 0.13507\n",
      "  Epoch 25/25 — train 0.08828 val 0.12486\n",
      "[TRAIN] ('PD', '90_degrees', 'step_turn')  (90 files)\n",
      "  Epoch 01/25 — train 0.70369 val 0.54980\n",
      "  Epoch 05/25 — train 0.23684 val 0.42833\n",
      "  Epoch 10/25 — train 0.16742 val 0.41713\n",
      "  Epoch 15/25 — train 0.13909 val 0.44953\n",
      "  Epoch 20/25 — train 0.12070 val 0.44165\n",
      "  Epoch 25/25 — train 0.10514 val 0.41333\n",
      "\n",
      "Saved model manifest: D:\\Courses\\thesis\\data\\second\\models_turning_norm_fixed\\_models_manifest.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   PD_or_C turning_angle type_of_turn  n_files  model_saved  \\\n",
       "0        C   135_degrees            -        1        False   \n",
       "1        C   135_degrees   pivot_turn       74         True   \n",
       "2        C   135_degrees    step_turn        3         True   \n",
       "3        C   180_degrees   pivot_turn      234         True   \n",
       "4        C   180_degrees    step_turn       12         True   \n",
       "5        C   225_degrees   pivot_turn        3         True   \n",
       "6        C   360_degrees   pivot_turn        1         True   \n",
       "7        C    90_degrees            -       10         True   \n",
       "8        C    90_degrees   pivot_turn      392         True   \n",
       "9        C    90_degrees    step_turn       13         True   \n",
       "10      PD   135_degrees            -        4         True   \n",
       "11      PD   135_degrees   pivot_turn       97         True   \n",
       "12      PD   135_degrees    step_turn       33         True   \n",
       "13      PD   180_degrees            -        4         True   \n",
       "14      PD   180_degrees   pivot_turn      209         True   \n",
       "15      PD   180_degrees    step_turn       76         True   \n",
       "16      PD   225_degrees    step_turn        2         True   \n",
       "17      PD   270_degrees   pivot_turn        1        False   \n",
       "18      PD    90_degrees            -       18         True   \n",
       "19      PD    90_degrees   pivot_turn      404         True   \n",
       "\n",
       "                                           model_path  \n",
       "0                                                None  \n",
       "1   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "2   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "3   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "4   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "5   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "6   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "7   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "8   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "9   D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "10  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "11  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "12  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "13  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "14  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "15  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "16  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "17                                               None  \n",
       "18  D:\\Courses\\thesis\\data\\second\\models_turning_n...  \n",
       "19  D:\\Courses\\thesis\\data\\second\\models_turning_n...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PD_or_C</th>\n",
       "      <th>turning_angle</th>\n",
       "      <th>type_of_turn</th>\n",
       "      <th>n_files</th>\n",
       "      <th>model_saved</th>\n",
       "      <th>model_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>74</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C</td>\n",
       "      <td>180_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>234</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C</td>\n",
       "      <td>180_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C</td>\n",
       "      <td>225_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C</td>\n",
       "      <td>360_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C</td>\n",
       "      <td>90_degrees</td>\n",
       "      <td>-</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C</td>\n",
       "      <td>90_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>392</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C</td>\n",
       "      <td>90_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>13</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PD</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>PD</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>97</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PD</td>\n",
       "      <td>135_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>33</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PD</td>\n",
       "      <td>180_degrees</td>\n",
       "      <td>-</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PD</td>\n",
       "      <td>180_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>209</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PD</td>\n",
       "      <td>180_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>76</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>PD</td>\n",
       "      <td>225_degrees</td>\n",
       "      <td>step_turn</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>PD</td>\n",
       "      <td>270_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>PD</td>\n",
       "      <td>90_degrees</td>\n",
       "      <td>-</td>\n",
       "      <td>18</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>PD</td>\n",
       "      <td>90_degrees</td>\n",
       "      <td>pivot_turn</td>\n",
       "      <td>404</td>\n",
       "      <td>True</td>\n",
       "      <td>D:\\Courses\\thesis\\data\\second\\models_turning_n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:07:25.231184Z",
     "start_time": "2025-08-14T17:07:25.217669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load a saved model (handles PyTorch 2.6+ weights_only change)\n",
    "import pickle\n",
    "\n",
    "def load_model_for_group(triple):\n",
    "    tag = \"__\".join(triple)\n",
    "    ckpt = MODEL_DIR / f\"TVAE_{tag}.pt\"\n",
    "    if not ckpt.exists():\n",
    "        raise FileNotFoundError(f\"No checkpoint for {triple}: {ckpt}\")\n",
    "\n",
    "    # Try normal load, then fallback to weights_only=False (safe if you trust your file)\n",
    "    try:\n",
    "        data = torch.load(ckpt, map_location=DEVICE)\n",
    "    except pickle.UnpicklingError:\n",
    "        data = torch.load(ckpt, map_location=DEVICE, weights_only=False)\n",
    "    except Exception as e:\n",
    "        # allowlist numpy reconstruct if needed\n",
    "        try:\n",
    "            from torch.serialization import add_safe_globals\n",
    "            import numpy as np\n",
    "            add_safe_globals([np.core.multiarray._reconstruct])\n",
    "            data = torch.load(ckpt, map_location=DEVICE)\n",
    "        except Exception:\n",
    "            raise e\n",
    "\n",
    "    model = TransformerVAE_BF(INPUT_DIM, win=data[\"meta\"][\"window\"]).to(DEVICE)\n",
    "    model.load_state_dict(data[\"model_state\"])\n",
    "    model.eval()\n",
    "    scaler = scaler_from_params(data[\"scaler_mean\"], data[\"scaler_scale\"])\n",
    "    header_cols = data.get(\"header_cols\", [f\"{ax}{i}\" for i in range(1, NUM_KPT+1) for ax in (\"x\",\"y\")])\n",
    "    return model, scaler, header_cols\n",
    "\n",
    "def generate_sequence_fixed(model, scaler, target_len, seed_seq=None, n_variants=1, temperature=0.95):\n",
    "    \"\"\"\n",
    "    Generate in NORMALIZED space; if 'seed_seq' provided, normalize by its frame-0\n",
    "    root & scale, then denormalize outputs back to those pixels so size stays constant.\n",
    "\n",
    "    MODIFIED: This version re-normalizes each generated frame to maintain a constant scale,\n",
    "    preventing the \"getting closer\" effect.\n",
    "    \"\"\"\n",
    "    variants = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_variants):\n",
    "            if seed_seq is not None and len(seed_seq) >= WINDOW:\n",
    "                seed_norm, root0, scale0 = normalize_sequence_xy(seed_seq)\n",
    "                seed_scaled = scaler.transform(seed_norm[:WINDOW])\n",
    "                seed_tensor = torch.tensor(seed_scaled[None], dtype=torch.float32, device=DEVICE)\n",
    "                mu, logvar = model.encode(seed_tensor)\n",
    "\n",
    "                current_window_norm = seed_norm[:WINDOW].copy()\n",
    "                out_frames = []\n",
    "                for _t in range(target_len):\n",
    "                    z = model.reparameterize(mu, logvar, temperature=temperature)\n",
    "                    synth_scaled = model.decode(z)[0].detach().cpu().numpy()\n",
    "                    frame_norm = scaler.inverse_transform(synth_scaled[-1][None])[0]\n",
    "\n",
    "                    # --- START: New code to fix the scaling issue ---\n",
    "                    # Calculate the scale of the newly generated frame in the normalized space.\n",
    "                    pts_norm = frame_norm.reshape(17, 2)\n",
    "                    current_frame_scale = _median_bone_scale_2d(pts_norm)\n",
    "\n",
    "                    # We want the scale to be 1.0, which was the scale of the first frame after normalization.\n",
    "                    # We re-scale the frame to enforce this constant size.\n",
    "                    if current_frame_scale > 1e-6:\n",
    "                        frame_norm_rescaled = frame_norm / current_frame_scale\n",
    "                    else:\n",
    "                        frame_norm_rescaled = frame_norm\n",
    "                    # --- END: New code ---\n",
    "\n",
    "                    # Use the RESCALED frame for both the output and the next prediction.\n",
    "                    frame_pix  = denormalize_frame_34(frame_norm_rescaled, root0, scale0)\n",
    "                    out_frames.append(frame_pix)\n",
    "\n",
    "                    current_window_norm = np.vstack([current_window_norm, frame_norm_rescaled])[-WINDOW:]\n",
    "                    win_scaled = scaler.transform(current_window_norm)\n",
    "                    mu, logvar = model.encode(torch.tensor(win_scaled[None], dtype=torch.float32, device=DEVICE))\n",
    "                variants.append(np.array(out_frames))\n",
    "\n",
    "            else:\n",
    "                # Seedless generation (logic remains the same, as it doesn't depend on a pre-existing scale)\n",
    "                latent_dim = model.fc_latent.in_features\n",
    "                z0 = torch.randn(1, latent_dim, device=DEVICE)\n",
    "                synth_scaled = model.decode(z0)[0].detach().cpu().numpy()\n",
    "                current_window_norm = scaler.inverse_transform(synth_scaled)\n",
    "\n",
    "                root0 = np.zeros(2, dtype=np.float32); scale0 = 1.0\n",
    "                out_frames = []\n",
    "                win_scaled = scaler.transform(current_window_norm[-WINDOW:])\n",
    "                mu, logvar = model.encode(torch.tensor(win_scaled[None], dtype=torch.float32, device=DEVICE))\n",
    "\n",
    "                for _t in range(target_len):\n",
    "                    z = model.reparameterize(mu, logvar, temperature=temperature)\n",
    "                    synth_scaled = model.decode(z)[0].detach().cpu().numpy()\n",
    "                    frame_norm = scaler.inverse_transform(synth_scaled[-1][None])[0]\n",
    "                    frame_pix  = denormalize_frame_34(frame_norm, root0, scale0)\n",
    "                    out_frames.append(frame_pix)\n",
    "\n",
    "                    current_window_norm = np.vstack([current_window_norm, frame_norm])[-WINDOW:]\n",
    "                    win_scaled = scaler.transform(current_window_norm)\n",
    "                    mu, logvar = model.encode(torch.tensor(win_scaled[None], dtype=torch.float32, device=DEVICE))\n",
    "\n",
    "                variants.append(np.array(out_frames))\n",
    "    return variants\n"
   ],
   "id": "88e22718c54dab97",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T17:07:25.638753Z",
     "start_time": "2025-08-14T17:07:25.289431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage: set to any trained triple from the manifest\n",
    "CHOSEN_GROUP = ('PD', '180_degrees', 'pivot_turn')   # <-- edit to a trained triple\n",
    "\n",
    "try:\n",
    "    model, scaler, header_cols = load_model_for_group(CHOSEN_GROUP)\n",
    "    group_path = GROUPED_DIR / CHOSEN_GROUP[0] / CHOSEN_GROUP[1] / CHOSEN_GROUP[2]\n",
    "    some_file = next(group_path.glob(\"*.csv\"))\n",
    "    seed_arr, _ = read_keypoints_csv(some_file, INPUT_DIM)\n",
    "\n",
    "    target_len = len(seed_arr)\n",
    "\n",
    "    # Seeded (keeps person size constant from the seed)\n",
    "    variants = generate_sequence_fixed(model, scaler, target_len=target_len, seed_seq=seed_arr,\n",
    "                                 n_variants=1, temperature=0.95)\n",
    "    synth = variants[0]\n",
    "    out_csv = SYN_DIR / f\"synthetic_{CHOSEN_GROUP[0]}__{CHOSEN_GROUP[1]}__{CHOSEN_GROUP[2]}_seeded_norm.csv\"\n",
    "    pd.DataFrame(synth, columns=header_cols).to_csv(out_csv, index=False, float_format=\"%.1f\")\n",
    "    print(\"Saved (seeded):\", out_csv)\n",
    "\n",
    "    # Seedless (unit-size stick figure at origin)\n",
    "    variants2 = generate_sequence_fixed(model, scaler, target_len=target_len, seed_seq=None,\n",
    "                                  n_variants=1, temperature=0.95)\n",
    "    synth2 = variants2[0]\n",
    "    out_csv2 = SYN_DIR / f\"synthetic_{CHOSEN_GROUP[0]}__{CHOSEN_GROUP[1]}__{CHOSEN_GROUP[2]}_seedless_norm.csv\"\n",
    "    pd.DataFrame(synth2, columns=header_cols).to_csv(out_csv2, index=False, float_format=\"%.1f\")\n",
    "    print(\"Saved (seedless):\", out_csv2)\n",
    "\n",
    "except StopIteration:\n",
    "    print(\"No CSV found in the chosen group's folder. Pick another CHOSEN_GROUP.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n"
   ],
   "id": "c05608db8b25694a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved (seeded): D:\\Courses\\thesis\\data\\second\\synthetic_turning_norm_fixed\\synthetic_PD__180_degrees__pivot_turn_seeded_norm.csv\n",
      "Saved (seedless): D:\\Courses\\thesis\\data\\second\\synthetic_turning_norm_fixed\\synthetic_PD__180_degrees__pivot_turn_seedless_norm.csv\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-14T18:22:53.962362Z",
     "start_time": "2025-08-14T18:22:52.010407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generate N seeded + seedless variants from one chosen group\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "CHOSEN_GROUP = ('PD', '90_degrees', 'pivot_turn')  # edit\n",
    "N_SEEDED_VARIANTS   = 10\n",
    "N_SEEDLESS_VARIANTS = 5\n",
    "TEMP = 0.9\n",
    "#LOCK_MODE = None   # \"per_frame_root\", \"anchor\", or None\n",
    "FIXED_SEED = None               # for reproducibility; set to None to randomize\n",
    "\n",
    "if FIXED_SEED is not None:\n",
    "    random.seed(FIXED_SEED); np.random.seed(FIXED_SEED); torch.manual_seed(FIXED_SEED)\n",
    "\n",
    "# load model + a seed file\n",
    "model, scaler, header_cols = load_model_for_group(CHOSEN_GROUP)\n",
    "group_path = GROUPED_DIR / CHOSEN_GROUP[0] / CHOSEN_GROUP[1] / CHOSEN_GROUP[2]\n",
    "seed_file = sorted(group_path.glob(\"*.csv\"))[0]  # or pick a specific one\n",
    "seed_arr, _ = read_keypoints_csv(seed_file, INPUT_DIM)\n",
    "target_len = len(seed_arr)\n",
    "\n",
    "print(\"Seed file:\", seed_file.name, \"frames:\", target_len)\n",
    "\n",
    "# ---- seeded variants ----\n",
    "for v in range(N_SEEDED_VARIANTS):\n",
    "    [synth] = generate_sequence_fixed(\n",
    "        model, scaler, target_len=target_len,\n",
    "        seed_seq=seed_arr, n_variants=1,\n",
    "        temperature=TEMP\n",
    "    )\n",
    "    out_csv = SYN_DIR / f\"synthetic_{CHOSEN_GROUP[0]}__{CHOSEN_GROUP[1]}__{CHOSEN_GROUP[2]}__seeded_v{v:02d}.csv\"\n",
    "    pd.DataFrame(synth, columns=header_cols).to_csv(out_csv, index=False, float_format=\"%.1f\")\n",
    "    print(\"Saved:\", out_csv.name)\n",
    "\n",
    "# ---- seedless variants ----\n",
    "for v in range(N_SEEDLESS_VARIANTS):\n",
    "    [synth] = generate_sequence_fixed(\n",
    "        model, scaler, target_len=target_len,\n",
    "        seed_seq=None, n_variants=1,\n",
    "        temperature=TEMP\n",
    "    )\n",
    "    out_csv = SYN_DIR / f\"synthetic_{CHOSEN_GROUP[0]}__{CHOSEN_GROUP[1]}__{CHOSEN_GROUP[2]}__seedless_v{v:02d}.csv\"\n",
    "    pd.DataFrame(synth, columns=header_cols).to_csv(out_csv, index=False, float_format=\"%.1f\")\n",
    "    print(\"Saved:\", out_csv.name)\n"
   ],
   "id": "d2be49c381db1ec9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed file: Pt253_PD_n_3452.csv frames: 64\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seeded_v00.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seeded_v01.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seeded_v02.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seeded_v03.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seeded_v04.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seeded_v05.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seeded_v06.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seeded_v07.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seeded_v08.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seeded_v09.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seedless_v00.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seedless_v01.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seedless_v02.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seedless_v03.csv\n",
      "Saved: synthetic_PD__90_degrees__pivot_turn__seedless_v04.csv\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
