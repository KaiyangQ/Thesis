{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Input File: E:/学习工作/PD/pks/SitToStand/Data/STS_2D_skeletons_coarsened/Pt204_C_n_301.csv\n",
      "开始读取 CSV 文件...\n",
      "CSV 文件读取完成，形状为: (508, 52)\n",
      "原始数据长度: 508\n",
      "初始化模型...\n",
      "Meta shape: (508, 2), Full synth shape: (508, 50)\n",
      "✔ 合成结果已保存：E:/学习工作/PD/pks/SitToStand/Data/Transformer_generate.csv （shape=(508, 52)）\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\python3.10.9\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# generate_single_sts.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ------------ 配置区 ------------\n",
    "# 待处理的单个输入 CSV\n",
    "INPUT_FILE = r'E:/学习工作/PD/pks/SitToStand/Data/STS_2D_skeletons_coarsened/Pt204_C_n_301.csv'\n",
    "# 生成后的保存路径（如果不填则自动在输入文件同目录下，加后缀 _synthetic）\n",
    "OUTPUT_FILE = r'E:/学习工作/PD/pks/SitToStand/Data/Transformer_generate.csv'  # 默认路径\n",
    "# 序列长度、关键点数量（x,y 对数）\n",
    "SEQ_LEN   = 50\n",
    "NUM_KPT   = 25\n",
    "INPUT_DIM = NUM_KPT * 2\n",
    "\n",
    "# 计算设备\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using Input File: {INPUT_FILE}\")\n",
    "\n",
    "# ------------ Positional Encoding ------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, D)\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "# ------------ Transformer VAE ------------\n",
    "class TransformerVAE(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, d_model=128, nhead=4, num_layers=2, latent_dim=64):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.input_linear = nn.Linear(input_dim, d_model)\n",
    "        self.pos_enc      = PositionalEncoding(d_model, max_len=seq_len)\n",
    "        encoder_layer     = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.encoder      = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc_mu        = nn.Linear(d_model*seq_len, latent_dim)\n",
    "        self.fc_logvar    = nn.Linear(d_model*seq_len, latent_dim)\n",
    "        self.fc_latent    = nn.Linear(latent_dim, d_model*seq_len)\n",
    "        decoder_layer     = nn.TransformerDecoderLayer(d_model, nhead)\n",
    "        self.decoder      = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.output_linear= nn.Linear(d_model, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x: (B, L, D)\n",
    "        h = self.input_linear(x)\n",
    "        h = self.pos_enc(h)\n",
    "        out = self.encoder(h.transpose(0,1)).transpose(0,1)\n",
    "        flat = out.contiguous().view(out.size(0), -1)\n",
    "        return self.fc_mu(flat), self.fc_logvar(flat)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        # z: (B, latent_dim)\n",
    "        x = self.fc_latent(z).view(z.size(0), self.seq_len, -1)\n",
    "        tgt = self.pos_enc(x)\n",
    "        memory = torch.zeros(self.seq_len, z.size(0), tgt.size(2), device=z.device)\n",
    "        out = self.decoder(tgt.transpose(0,1), memory).transpose(0,1)\n",
    "        return self.output_linear(out)\n",
    "\n",
    "\n",
    "# ------------ 主流程 ------------\n",
    "def main():\n",
    "    global OUTPUT_FILE  # 声明 OUTPUT_FILE 为全局变量\n",
    "\n",
    "    # 1. 读入 CSV\n",
    "    print(\"开始读取 CSV 文件...\")\n",
    "    df = pd.read_csv(INPUT_FILE, header=2, engine='python', on_bad_lines='skip')\n",
    "    print(f\"CSV 文件读取完成，形状为: {df.shape}\")\n",
    "\n",
    "    # 保留前两列（一般是 frame, timestamp 或其他），后面开始是关键点\n",
    "    meta = df.iloc[:, :2].reset_index(drop=True)\n",
    "    data_np = df.iloc[:, 2:2+INPUT_DIM].to_numpy(dtype=np.float32)\n",
    "    orig_len = data_np.shape[0]\n",
    "    print(f\"原始数据长度: {orig_len}\")\n",
    "\n",
    "    if orig_len < SEQ_LEN:\n",
    "        pad = np.zeros((SEQ_LEN - orig_len, INPUT_DIM), dtype=np.float32)\n",
    "        data_for_model = np.vstack([data_np, pad])\n",
    "    else:\n",
    "        data_for_model = data_np[:SEQ_LEN]\n",
    "\n",
    "    # 2. 初始化并加载（或训练）模型\n",
    "    print(\"初始化模型...\")\n",
    "    model = TransformerVAE(INPUT_DIM, SEQ_LEN).to(DEVICE)\n",
    "    model.to(DEVICE).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = torch.from_numpy(data_for_model[None]).to(DEVICE)\n",
    "        mu, logvar = model.encode(x)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "        synth = model.decode(z)[0].cpu().numpy()\n",
    "\n",
    "    # ========== 4. 按原始帧长“反填”或截断合成结果 ==========\n",
    "    if orig_len > SEQ_LEN:\n",
    "        pad_back = np.zeros((orig_len - SEQ_LEN, INPUT_DIM), dtype=np.float32)\n",
    "        full_synth = np.vstack([synth, pad_back])\n",
    "    else:\n",
    "        full_synth = synth[:orig_len]\n",
    "\n",
    "    # ========== 5. 拼回 meta 并保存 ==========\n",
    "    print(f\"Meta shape: {meta.shape}, Full synth shape: {full_synth.shape}\")\n",
    "    if meta.shape[0] != full_synth.shape[0]:\n",
    "        if meta.shape[0] > full_synth.shape[0]:\n",
    "            pad_back = np.zeros((meta.shape[0] - full_synth.shape[0], INPUT_DIM), dtype=np.float32)\n",
    "            full_synth = np.vstack([full_synth, pad_back])\n",
    "        else:\n",
    "            full_synth = full_synth[:meta.shape[0]]\n",
    "\n",
    "    out_arr = np.hstack([meta.values, full_synth])\n",
    "    out_cols = list(meta.columns) + list(df.columns[2:2+INPUT_DIM])\n",
    "    out_df = pd.DataFrame(out_arr, columns=out_cols)\n",
    "\n",
    "    if OUTPUT_FILE is None or OUTPUT_FILE == '':\n",
    "        base, _ = os.path.splitext(INPUT_FILE)\n",
    "        OUTPUT_FILE = f\"{base}_synthetic.csv\"\n",
    "\n",
    "    out_df.to_csv(OUTPUT_FILE, header=False, index=False, float_format='%.6f')\n",
    "    print(f\"✔ 合成结果已保存：{OUTPUT_FILE} （shape={out_df.shape}）\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] 目录名称无效。: 'E:/学习工作/PD/pks/SitToStand/Data/Transformer_generate.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 27\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(OUTPUT_DIR) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(INPUT_FILE):\n\u001b[0;32m     24\u001b[0m     base_for_filter \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(INPUT_FILE))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     25\u001b[0m     synthetic_file_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, f)\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(base_for_filter)  \u001b[38;5;66;03m# Filter for related synthetic files\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     ]\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m synthetic_file_list:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo synthetic files found in the output directory for the current input file.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] 目录名称无效。: 'E:/学习工作/PD/pks/SitToStand/Data/Transformer_generate.csv'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Configuration for Plotting ---\n",
    "# Select a few keypoints to plot (indices based on 0 to 16 for 17 keypoints)\n",
    "keypoints_to_plot = [0, 8, 16]  # Example: Nose (0), MidHip (8 if it exists), Right Ankle (16)\n",
    "num_samples_to_plot = 3  # How many synthetic samples to plot (will create this many separate figures/plot sets)\n",
    "\n",
    "# Define plot styles\n",
    "color_original = 'blue'\n",
    "color_synthetic = 'green'\n",
    "linewidth_original = 2\n",
    "linestyle_synthetic = '--'\n",
    "\n",
    "# --- Load synthetic samples for comparison ---\n",
    "OUTPUT_DIR = r'E:/学习工作/PD/pks/SitToStand/Data/Transformer_generate.csv'  # Ensure this matches the output directory\n",
    "INPUT_FILE = r'E:/学习工作/PD/pks/SitToStand/Data/STS_2D_skeletons_coarsened/Pt204_C_n_301.csv'\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR) and os.path.exists(INPUT_FILE):\n",
    "\n",
    "    base_for_filter = os.path.splitext(os.path.basename(INPUT_FILE))[0]\n",
    "    synthetic_file_list = [\n",
    "        os.path.join(OUTPUT_DIR, f)\n",
    "        for f in os.listdir(OUTPUT_DIR)\n",
    "        if f.endswith('.csv') and f.startswith(base_for_filter)  # Filter for related synthetic files\n",
    "    ]\n",
    "\n",
    "    if not synthetic_file_list:\n",
    "        print(\"No synthetic files found in the output directory for the current input file.\")\n",
    "    else:\n",
    "        # Select random synthetic samples to plot, or plot all up to num_samples_to_plot\n",
    "        paths_to_plot = random.sample(synthetic_file_list, min(num_samples_to_plot, len(synthetic_file_list)))\n",
    "\n",
    "        # Load the original sequence\n",
    "        df_original = pd.read_csv(INPUT_FILE, header=2)\n",
    "        original_data = df_original.to_numpy()\n",
    "        num_frames_original = original_data.shape[0]\n",
    "        time_steps_original = np.arange(num_frames_original)\n",
    "\n",
    "        print(f\"\\nPlotting original vs. {len(paths_to_plot)} individual synthetic samples...\")\n",
    "\n",
    "        for idx, path in enumerate(paths_to_plot):\n",
    "            try:\n",
    "                df_synthetic = pd.read_csv(path, header=2)\n",
    "                synthetic_sequence = df_synthetic.to_numpy()\n",
    "                sample_name = os.path.basename(path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading synthetic file {path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Create a new figure for each original vs. synthetic comparison\n",
    "            num_keypoints = len(keypoints_to_plot)\n",
    "            fig, axes = plt.subplots(num_keypoints, 2,\n",
    "                                     figsize=(14, 3.5 * num_keypoints),\n",
    "                                     sharex=True)\n",
    "            if num_keypoints == 1:  # Adjust axes array if only one keypoint is plotted\n",
    "                axes = np.array([axes])\n",
    "\n",
    "            fig.suptitle(f'Original vs. Synthetic: {sample_name}', fontsize=16, y=1.02)  # Adjust y for title\n",
    "\n",
    "            for i, kp_index in enumerate(keypoints_to_plot):\n",
    "                x_index = kp_index * 2\n",
    "                y_index = kp_index * 2 + 1\n",
    "\n",
    "                if x_index >= original_data.shape[1] or y_index >= original_data.shape[1]:\n",
    "                    print(f\"Warning: Keypoint index {kp_index} is out of bounds for original data. Skipping.\")\n",
    "                    if axes.ndim > 1: axes[i, 0].set_visible(False); axes[i, 1].set_visible(False)  # Hide unused subplot\n",
    "                    else: axes[0].set_visible(False); axes[1].set_visible(False)\n",
    "                    continue\n",
    "                if x_index >= synthetic_sequence.shape[1] or y_index >= synthetic_sequence.shape[1]:\n",
    "                    print(f\"Warning: Keypoint index {kp_index} is out of bounds for synthetic data {sample_name}. Skipping.\")\n",
    "                    if axes.ndim > 1: axes[i, 0].set_visible(False); axes[i, 1].set_visible(False)\n",
    "                    else: axes[0].set_visible(False); axes[1].set_visible(False)\n",
    "                    continue\n",
    "\n",
    "                # Plot Original Data\n",
    "                axes[i, 0].plot(time_steps_original, original_data[:, x_index],\n",
    "                                label=f'Original KP {kp_index} (X)', color=color_original, linewidth=linewidth_original)\n",
    "                axes[i, 1].plot(time_steps_original, original_data[:, y_index],\n",
    "                                label=f'Original KP {kp_index} (Y)', color=color_original, linewidth=linewidth_original)\n",
    "\n",
    "                # Plot Current Synthetic Sample\n",
    "                num_synth_frames = synthetic_sequence.shape[0]\n",
    "                time_steps_synthetic = np.arange(num_synth_frames)\n",
    "                axes[i, 0].plot(time_steps_synthetic, synthetic_sequence[:, x_index],\n",
    "                                label=f'Synth (X)', linestyle=linestyle_synthetic, color=color_synthetic, alpha=0.9)\n",
    "                axes[i, 1].plot(time_steps_synthetic, synthetic_sequence[:, y_index],\n",
    "                                label=f'Synth (Y)', linestyle=linestyle_synthetic, color=color_synthetic, alpha=0.9)\n",
    "\n",
    "                axes[i, 0].set_ylabel(f'KP {kp_index} X-Coord')\n",
    "                axes[i, 1].set_ylabel(f'KP {kp_index} Y-Coord')\n",
    "                axes[i, 0].legend(loc='best', fontsize='x-small')\n",
    "                axes[i, 1].legend(loc='best', fontsize='x-small')\n",
    "                axes[i, 0].grid(True, linestyle=':', alpha=0.7)\n",
    "                axes[i, 1].grid(True, linestyle=':', alpha=0.7)\n",
    "\n",
    "            if num_keypoints > 0:  # Only set xlabel if plots were made\n",
    "                axes[-1, 0].set_xlabel('Frame Number')\n",
    "                axes[-1, 1].set_xlabel('Frame Number')\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.97])  # Adjust layout\n",
    "            plt.show()  # Show the figure for the current synthetic sample\n",
    "\n",
    "        print(\"\\nPlotting complete.\")\n",
    "        print(\"If synthetic lines are still very similar to original, consider increasing augmentation strengths (e.g., JITTER_SCALE).\")\n",
    "else:\n",
    "    print(\"Error: OUTPUT_DIR or INPUT_FILE not defined or not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
